import time
import random
import re
import pandas as pd
import urllib.parse

# Selenium ê´€ë ¨ Import
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import TimeoutException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager

# BeautifulSoup Import
from bs4 import BeautifulSoup


# BASE URL ì •ì˜
BASE_URL = "https://arca.live" 

# User-Agent ëª©ë¡ ì •ì˜(ëœë¤ì„ íƒ)
USER_AGENT_LIST = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 14_2_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
]

# robots.txtì— ëª…ì‹œëœ í¬ë¡¤ë§ ê¸ˆì§€(Disallow) ì±„ë„ ID ëª©ë¡ ì •ì˜
DISALLOWED_CHANNEL_IDS = {'my'} 

def extract_arca_comments(soup):
    """
    ArcaLive ê²Œì‹œë¬¼ì—ì„œ ëŒ“ê¸€ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    comments_formatted = ""
    extracted_comments = []
    
    # ëŒ“ê¸€ ì•„ì´í…œë“¤ ì°¾ê¸°
    comment_items = soup.select('div.comment-item')
    
    for item in comment_items:
        # ëŒ€ëŒ“ê¸€ ë“±ì„ í¬í•¨í•˜ì—¬ í…ìŠ¤íŠ¸ ì˜ì—­ ì°¾ê¸°
        text_div = item.select_one('div.message div.text')
        if text_div:
            c_text = text_div.get_text('\n', strip=True)
            
            # "ì‚­ì œëœ ëŒ“ê¸€ì…ë‹ˆë‹¤" í•„í„°ë§
            if c_text and "ì‚­ì œëœ ëŒ“ê¸€ì…ë‹ˆë‹¤" not in c_text:
                extracted_comments.append(c_text)
                
    if extracted_comments:
        comments_formatted = " ||| ".join(extracted_comments)
        
    return comments_formatted

def search_arca(channel_id: str = 'breaking', search_keyword: str = "", start_page: int = 1, end_page: int = 1) -> pd.DataFrame:
    """
    ì•„ì¹´ë¼ì´ë¸Œ ì±„ë„ ëª©ë¡ ë° ì±„ë„ ë‚´ ê²€ìƒ‰, í†µí•© ê²€ìƒ‰(channel_id='breaking' ì‚¬ìš©)ì„ Seleniumì„ ì‚¬ìš©í•˜ì—¬ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    ê²Œì‹œê¸€ ë³¸ë¬¸ê³¼ í•¨ê»˜ í…ìŠ¤íŠ¸ ëŒ“ê¸€ì„ ìˆ˜ì§‘í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
    """
    
    data_list = []
    
    # robots.txt disallow ì±„ë„ í•„í„°ë§
    if channel_id in DISALLOWED_CHANNEL_IDS:
        print(f"\nğŸš¨ ê²½ê³ : ì±„ë„ ID '{channel_id}'ëŠ” robots.txtì— ì˜í•´ í¬ë¡¤ë§ì´ ê¸ˆì§€ëœ IDì…ë‹ˆë‹¤. ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")

        data_list.append({
            'Site': 'ARCALIVE',
            'PostID': 'robots.txt disallow',
            'Title': 'robots.txt disallow',
            'Content': f"\nğŸš¨ ê²½ê³ : ì±„ë„ ID '{channel_id}'ëŠ” robots.txtì— ì˜í•´ í¬ë¡¤ë§ì´ ê¸ˆì§€ëœ IDì…ë‹ˆë‹¤. ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.",
            'Comments': 'robots.txt disallow',
            'GalleryID': 'robots.txt disallow', 
            'PostURL': 'robots.txt disallow'
        })

        return pd.DataFrame(data_list)
    
    # WebDriver ì„¤ì •
    options = webdriver.ChromeOptions()
    options.add_argument('headless')
    options.add_argument('disable-gpu')
    options.add_argument('log-level=3')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument(f'user-agent={random.choice(USER_AGENT_LIST)}')
    
    # [ìµœì í™” 1] í˜ì´ì§€ ë¡œë“œ ì „ëµ: 'eager' (DOM ë¡œë“œ ì‹œì ê¹Œì§€ë§Œ ëŒ€ê¸°)
    options.page_load_strategy = 'eager'

    # [ìµœì í™” 2] ì´ë¯¸ì§€ ë° ë¶ˆí•„ìš”í•œ ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨
    prefs = {
        "profile.managed_default_content_settings.images": 2,  # ì´ë¯¸ì§€ ì°¨ë‹¨
        "profile.default_content_setting_values.notifications": 2,
        "profile.managed_default_content_settings.stylesheets": 2,
        "profile.managed_default_content_settings.cookies": 1,
        "profile.managed_default_content_settings.javascript": 1,
        "profile.managed_default_content_settings.plugins": 1,
        "profile.managed_default_content_settings.popups": 2,
        "profile.managed_default_content_settings.geolocation": 2,
        "profile.managed_default_content_settings.media_stream": 2,
    }

    options.add_experimental_option("prefs", prefs)

    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    
    driver = None
    try:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        # eager ëª¨ë“œì´ë¯€ë¡œ íƒ€ì„ì•„ì›ƒì„ 20ì´ˆë¡œ ë‹¨ì¶•
        driver.set_page_load_timeout(20)
        print("[DEBUG] Arca WebDriver ì´ˆê¸°í™” ì„±ê³µ")
    except Exception as e:
        print(f"\nâŒ WebDriver ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return pd.DataFrame(data_list)
    
    is_breaking_channel = channel_id == 'breaking'

    try:
        for i in range(int(start_page), int(end_page) + 1):
            
            # ----------------------
            # 1ë‹¨ê³„: ëª©ë¡ í˜ì´ì§€ ìš”ì²­ URL êµ¬ì„± ë° ë¡œë”©
            # ----------------------
            
            BASE_CHANNEL_URL = f"{BASE_URL}/b/{channel_id}"
            
            # ê²€ìƒ‰ì–´ ìœ ë¬´ì— ë”°ë¥¸ URL íŒŒë¼ë¯¸í„° êµ¬ì„±
            if search_keyword:
                params = {'target': 'all', 'keyword': search_keyword, 'p': i}
            else:
                params = {'p': i}
            
            full_url = BASE_CHANNEL_URL + '?' + urllib.parse.urlencode(params)
            
            print(f"--- [ARCA] ëª©ë¡ í˜ì´ì§€ {i} ì§„ì…. ì±„ë„ '{channel_id}', ê²€ìƒ‰ì–´: {search_keyword}, URL: {full_url} ---")
            
            # Seleniumìœ¼ë¡œ í˜ì´ì§€ ë¡œë“œ
            driver.get(full_url)
            
            # í˜ì´ì§€ê°€ ì™„ì „íˆ ë¡œë“œë  ë•Œê¹Œì§€ ëª…ì‹œì ìœ¼ë¡œ ê¸°ë‹¤ë¦¼
            try:
                # ê²Œì‹œë¬¼ ëª©ë¡ì˜ ì²« ë²ˆì§¸ í•­ëª©(a.vrow.column ë˜ëŠ” div.vrow.hybrid)ì´ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ìµœëŒ€ 15ì´ˆ ëŒ€ê¸°
                WebDriverWait(driver, 15).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, 'div.list-table a.vrow.column, div.list-table div.vrow.hybrid'))
                )
            except TimeoutException:
                print(f"[ARCA] í˜ì´ì§€ {i} ë¡œë“œ ì‹œê°„ ì´ˆê³¼. ìœ íš¨í•œ ê²Œì‹œë¬¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.")
                break
            
            # ë¡œë“œëœ HTMLì„ BeautifulSoupìœ¼ë¡œ íŒŒì‹±
            soup = BeautifulSoup(driver.page_source, 'lxml')
            
            # [í†µí•© ì„ íƒì ì ìš©]
            article_list = soup.select(
                'div.list-table a.vrow.column:not(.notice), '
                'div.list-table div.vrow.hybrid:not(.notice) a.hybrid-title'
            )
            
            if not article_list:
                print(f"[ARCA] í˜ì´ì§€ {i}ì—ì„œ ìœ íš¨í•œ ì¼ë°˜ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.")
                break 

            print(f"-> [ARCA] í˜ì´ì§€ {i}ì—ì„œ {len(article_list)}ê°œì˜ ê²Œì‹œë¬¼ ëª©ë¡ í™•ë³´.")
            
            # ----------------------
            # 2ë‹¨ê³„: ê°œë³„ ê²Œì‹œë¬¼ ì ‘ê·¼ ë° ë‚´ìš© ì¶”ì¶œ 
            # ----------------------
            for a_item in article_list:
                
                relative_url = a_item.get('href')
                
                # URLì—ì„œ ê²Œì‹œë¬¼ ë²ˆí˜¸(PostID) ì¶”ì¶œ
                post_id_match = re.search(r'/(\d+)(?:\?|$)', relative_url)
                post_id = post_id_match.group(1) if post_id_match else None

                if not post_id: continue
                
                # ì œëª© ì¶”ì¶œ
                title_tag = a_item.select_one('span.title')
                title_raw = title_tag.get_text(strip=True) if title_tag else a_item.get_text(strip=True)

                # ê²Œì‹œë¬¼ ì „ì²´ URL
                post_full_url = BASE_URL + relative_url

                # GalleryID (ì±„ë„ ì •ë³´) ê²°ì • ë¡œì§
                gallery_id_for_output = channel_id 
                
                if is_breaking_channel:
                    # í˜„ì¬ í–‰(a_item) ë‚´ë¶€ì—ì„œ ë°°ì§€ ì°¾ê¸°
                    badge_tag = a_item.select_one('span.badge')
                    if badge_tag:
                        gallery_id_for_output = badge_tag.get_text(strip=True)
                    else:
                        gallery_id_for_output = "Unknown Channel"
                
                time.sleep(random.uniform(1.5, 3.5)) 
                
                # ê²Œì‹œë¬¼ ë³¸ë¬¸ ìš”ì²­
                article_contents = ""
                comments_formatted = ""

                try:
                    print(f"    -> [ARCA] ê²Œì‹œë¬¼ ë³¸ë¬¸ ìš”ì²­: {title_raw[:20]}... (ID: {post_id}, ì±„ë„: {gallery_id_for_output})")
                    driver.get(post_full_url) 
                    
                    WebDriverWait(driver, 5).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, 'div.article-content'))
                    )

                    # ëŒ“ê¸€ ì˜ì—­ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸° (div.comment-itemì´ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€)
                    try:
                        WebDriverWait(driver, 1).until(
                            EC.presence_of_element_located((By.CSS_SELECTOR, 'div#comment'))
                        )
                    except TimeoutException:
                        print("[ARCA] ëŒ“ê¸€ ë¡œë”© ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    
                    article_soup = BeautifulSoup(driver.page_source, 'lxml')

                    # 1. ë³¸ë¬¸ ì¶”ì¶œ
                    article_contents_tag = article_soup.find('div', class_='article-content')
                    if article_contents_tag:
                        article_contents = article_contents_tag.get_text('\n', strip=True)
                    
                    # 2. ëŒ“ê¸€ ì¶”ì¶œ
                    comments_formatted = extract_arca_comments(article_soup)

                except TimeoutException:
                    print(f"    -> [ARCA] ê²Œì‹œë¬¼ ë³¸ë¬¸ ë¡œë“œ ì‹œê°„ ì´ˆê³¼ ({post_full_url}). ë³¸ë¬¸/ëŒ“ê¸€ ìˆ˜ì§‘ ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue 
                except Exception as e:
                    print(f"    -> [ARCA] ê²Œì‹œë¬¼ ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ({post_full_url}): {e}")
                    continue
                
                # ----------------------
                # 3ë‹¨ê³„: ë°ì´í„° í´ë¦¬ë‹ ë° ì €ì¥
                # ----------------------
                
                pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$\-@\.&+:/?=]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
                repl = ''
                title_clean = re.sub(pattern=pattern, repl=repl, string=title_raw).strip()
                article_contents_clean = re.sub(pattern=pattern, repl=repl, string=article_contents).strip()
                
                if article_contents_clean:
                    data_list.append({
                        'Site': 'ARCALIVE',
                        'PostID': post_id,
                        'Title': title_clean,
                        'Content': article_contents_clean,
                        'Comments': comments_formatted,
                        'GalleryID': gallery_id_for_output, 
                        'PostURL': post_full_url
                    })

    finally:
        if driver:
            driver.quit()
            print("--- WebDriver ì¢…ë£Œ ---")

    # ----------------------
    # 4ë‹¨ê³„: ë¦¬ìŠ¤íŠ¸ë¥¼ ìµœì¢… DataFrameìœ¼ë¡œ ë³€í™˜ ë° ì¤‘ë³µ ì œê±°
    # ----------------------
    df = pd.DataFrame(data_list)

    # PostIDë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ í–‰ ì œê±° 
    if not df.empty:
        df = df.drop_duplicates(subset=['GalleryID', 'PostID'], keep='first')
        print(f"\n--- [ARCA] í¬ë¡¤ë§ ì™„ë£Œ ë° ì¤‘ë³µ ì œê±° ---")
        print(f"ì´ ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ ìˆ˜ (ì›ë³¸): {len(data_list)}ê°œ")
        print(f"ì¤‘ë³µ ì œê±° í›„ ìµœì¢… ê²Œì‹œë¬¼ ìˆ˜: {len(df)}ê°œ")
            
    return df