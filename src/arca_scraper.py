import time 	# ëœë¤ ë”œë ˆì´ì‹œ
import random 	# ëœë¤ ë”œë ˆì´ì‹œ
import re 	# ì •ê·œ í‘œí˜„ì‹
import pandas as pd # Pandas df ì‚¬ìš©
import urllib.parse # URL ì¸ì½”ë”©ìš©

# Selenium ê´€ë ¨ Import
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException

# BeautifulSoup Import (HTML íŒŒì‹±ì€ ìœ ì§€)
from bs4 import BeautifulSoup


# BASE URL ì •ì˜ (ìµœìƒìœ„ ë„ë©”ì¸ìœ¼ë¡œ í†µí•©)
BASE_URL = "https://arca.live" 

# User-Agent ëª©ë¡ ì •ì˜(ëœë¤ì„ íƒ)
USER_AGENT_LIST = [
	'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36',
	'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36',
	'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36',
	'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36',
	'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
	'Mozilla/5.0 (Macintosh; Intel Mac OS X 14_2_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
]

# robots.txtì— ëª…ì‹œëœ í¬ë¡¤ë§ ê¸ˆì§€(Disallow) ì±„ë„ ID ëª©ë¡ ì •ì˜
DISALLOWED_CHANNEL_IDS = {'my'} 

def get_arca_posts(channel_id: str, search_keyword: str = "", start_page: int = 1, end_page: int = 3) -> pd.DataFrame:
	"""
	ì•„ì¹´ë¼ì´ë¸Œ ì±„ë„ ëª©ë¡ ë° ì±„ë„ ë‚´ ê²€ìƒ‰, í†µí•© ê²€ìƒ‰(channel_id='breaking' ì‚¬ìš©)ì„ Seleniumì„ ì‚¬ìš©í•˜ì—¬ ìˆ˜í–‰í•©ë‹ˆë‹¤.
	ê²Œì‹œê¸€ ë³¸ë¬¸ê³¼ í•¨ê»˜ í…ìŠ¤íŠ¸ ëŒ“ê¸€ì„ ìˆ˜ì§‘í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
	
	:param channel_id: í¬ë¡¤ë§í•  ì•„ì¹´ë¼ì´ë¸Œ ì±„ë„ ID (ì˜ˆ: 'wutheringwaves' ë˜ëŠ” í†µí•© ê²€ìƒ‰ìš© 'breaking', 'hotdeal')
	:param search_keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ (ì„ íƒ ì‚¬í•­)
	:param start_page: ì‹œì‘ í˜ì´ì§€
	:param end_page: ì¢…ë£Œ í˜ì´ì§€
	:return: ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë‹´ì€ Pandas DataFrame
	"""
	
	data_list = []
	
	# robots.txt disallow ì±„ë„ í•„í„°ë§
	if channel_id in DISALLOWED_CHANNEL_IDS:
		print(f"\nğŸš¨ ê²½ê³ : ì±„ë„ ID '{channel_id}'ëŠ” robots.txtì— ì˜í•´ í¬ë¡¤ë§ì´ ê¸ˆì§€ëœ IDì…ë‹ˆë‹¤. ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")

		data_list.append({
						'Site': 'ARCALIVE',
						'PostID': 'robots.txt disallow',
						'Title': 'robots.txt disallow',
						'Content': f"\nğŸš¨ ê²½ê³ : ì±„ë„ ID '{channel_id}'ëŠ” robots.txtì— ì˜í•´ í¬ë¡¤ë§ì´ ê¸ˆì§€ëœ IDì…ë‹ˆë‹¤. ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.",
						'Comments': 'robots.txt disallow',
						'GalleryID': 'robots.txt disallow', 
						'PostURL': 'robots.txt disallow'
					})

		return pd.DataFrame(data_list)
	
	# WebDriver ì„¤ì •
	options = webdriver.ChromeOptions()
	options.add_argument('headless') # ë¸Œë¼ìš°ì € ì°½ì„ ë„ìš°ì§€ ì•ŠëŠ” í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ
	options.add_argument('disable-gpu') # GPU ì‚¬ìš© ì•ˆ í•¨ (í—¤ë“œë¦¬ìŠ¤ í™˜ê²½ì—ì„œ í•„ìš”)
	options.add_argument('log-level=3') # ë¡œê·¸ ë ˆë²¨ ì„¤ì • (ë¶ˆí•„ìš”í•œ ë©”ì‹œì§€ ì œê±°)
	
	# [ì¤‘ìš”] Headless ê°ì§€ ë°©ì§€ë¥¼ ìœ„í•´ User-Agent ì„¤ì • í•„ìˆ˜
	options.add_argument(f'user-agent={random.choice(USER_AGENT_LIST)}') 
	
	driver = None
	try:
		# Selenium Managerê°€ ìë™ìœ¼ë¡œ ë“œë¼ì´ë²„ë¥¼ ì°¾ì•„ ì„¤ì¹˜í•©ë‹ˆë‹¤.
		driver = webdriver.Chrome(options=options)
	except WebDriverException as e:
		print(f"\nâŒ WebDriver ì´ˆê¸°í™” ì‹¤íŒ¨. Chrome ì„¤ì¹˜ ë° ë“œë¼ì´ë²„ í˜¸í™˜ì„±ì„ í™•ì¸í•˜ì„¸ìš”.")
		print(f"ì˜¤ë¥˜: {e}")
		return pd.DataFrame(data_list)
	
	is_breaking_channel = channel_id == 'breaking'

	try:
		for i in range(start_page, end_page + 1):
			
			# ----------------------
			# 1ë‹¨ê³„: ëª©ë¡ í˜ì´ì§€ ìš”ì²­ URL êµ¬ì„± ë° ë¡œë”©
			# ----------------------
			
			BASE_CHANNEL_URL = f"{BASE_URL}/b/{channel_id}"
			
			# ê²€ìƒ‰ì–´ ìœ ë¬´ì— ë”°ë¥¸ URL íŒŒë¼ë¯¸í„° êµ¬ì„± ë¶„ë¦¬ (ì‚¬ìš©ì ìš”ì²­ ë°˜ì˜)
			if search_keyword:
				# ê²€ìƒ‰ì–´ê°€ ìˆëŠ” ê²½ìš°: target=all í¬í•¨
				params = {'target': 'all', 'keyword': search_keyword, 'p': i}
			else:
				# ê²€ìƒ‰ì–´ê°€ ì—†ëŠ” ê²½ìš° (ì¼ë°˜ ì ‘ì†): í˜ì´ì§€ë§Œ ì§€ì • (?p=1)
				params = {'p': i}
			
			full_url = BASE_CHANNEL_URL + '?' + urllib.parse.urlencode(params)
			
			print(f"--- ì±„ë„ '{channel_id}' ëª©ë¡ í˜ì´ì§€ {i} ìš”ì²­ ì¤‘: {full_url} ---")
			
			# Seleniumìœ¼ë¡œ í˜ì´ì§€ ë¡œë“œ
			driver.get(full_url)
			
			# í˜ì´ì§€ê°€ ì™„ì „íˆ ë¡œë“œë  ë•Œê¹Œì§€ ëª…ì‹œì ìœ¼ë¡œ ê¸°ë‹¤ë¦¼
			try:
				# ê²Œì‹œë¬¼ ëª©ë¡ì˜ ì²« ë²ˆì§¸ í•­ëª©(a.vrow.column ë˜ëŠ” div.vrow.hybrid)ì´ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ìµœëŒ€ 15ì´ˆ ëŒ€ê¸°
				WebDriverWait(driver, 15).until(
					EC.presence_of_element_located((By.CSS_SELECTOR, 'div.list-table a.vrow.column, div.list-table div.vrow.hybrid'))
				)
			except TimeoutException:
				print(f"í˜ì´ì§€ {i} ë¡œë“œ ì‹œê°„ ì´ˆê³¼. ìœ íš¨í•œ ê²Œì‹œë¬¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.")
				break
			
			# ë¡œë“œëœ HTMLì„ BeautifulSoupìœ¼ë¡œ íŒŒì‹±
			soup = BeautifulSoup(driver.page_source, 'lxml')
			
			# [í†µí•© ì„ íƒì ì ìš©]
			# 1. ì¼ë°˜ ì±„ë„ ê²Œì‹œë¬¼ ë§í¬: a.vrow.column:not(.notice)
			# 2. í•«ë”œ ì±„ë„ ê²Œì‹œë¬¼ ë§í¬: div.vrow.hybrid:not(.notice) ë‚´ë¶€ì˜ a.hybrid-title
			# ì´ ë‘ ê²½ìš° ëª¨ë‘ hrefë¥¼ ê°€ì§„ <a> íƒœê·¸ë¥¼ ì„ íƒí•˜ë©°, ê³µì§€ê¸€(.notice)ì€ ì œì™¸í•©ë‹ˆë‹¤.
			article_list = soup.select(
				'div.list-table a.vrow.column:not(.notice), '
				'div.list-table div.vrow.hybrid:not(.notice) a.hybrid-title'
			)
			
			if not article_list:
				print(f"í˜ì´ì§€ {i}ì—ì„œ ìœ íš¨í•œ ì¼ë°˜ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.")
				break 

			print(f"-> í˜ì´ì§€ {i}ì—ì„œ {len(article_list)}ê°œì˜ ê²Œì‹œë¬¼ ëª©ë¡ í™•ë³´.")
			
			# ----------------------
			# 2ë‹¨ê³„: ê°œë³„ ê²Œì‹œë¬¼ ì ‘ê·¼ ë° ë‚´ìš© ì¶”ì¶œ 
			# ----------------------
			for a_item in article_list:
				
				# a_itemì€ ì´ì œ í•­ìƒ hrefë¥¼ ê°€ì§„ <a> íƒœê·¸ì…ë‹ˆë‹¤.
				relative_url = a_item.get('href')
				
				# URLì—ì„œ ê²Œì‹œë¬¼ ë²ˆí˜¸(PostID) ì¶”ì¶œ
				post_id_match = re.search(r'/(\d+)(?:\?|$)', relative_url)
				
				post_id = post_id_match.group(1) if post_id_match else None

				if not post_id:
					continue
				
				# ì œëª© ì¶”ì¶œ (hotdealì˜ ê²½ìš° hybrid-title ë‚´ì˜ í…ìŠ¤íŠ¸ê°€ ì œëª©ì´ ë©ë‹ˆë‹¤.)
				title_tag = a_item.select_one('span.title')
				title_raw = title_tag.get_text(strip=True) if title_tag else a_item.get_text(strip=True) # span.titleì´ ì—†ìœ¼ë©´ <a> íƒœê·¸ ìì²´ í…ìŠ¤íŠ¸ ì‚¬ìš©

				# ê²Œì‹œë¬¼ ì „ì²´ URL
				post_full_url = BASE_URL + relative_url

				# GalleryID (ì±„ë„ ì •ë³´) ê²°ì • ë¡œì§
				gallery_id_for_output = channel_id 

				if is_breaking_channel:
					# í†µí•© ê²€ìƒ‰ì¸ ê²½ìš°: ë°°ì§€(badge.badge-success)ì—ì„œ ì±„ë„ ì´ë¦„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
					# í†µí•© ê²€ìƒ‰ ê²°ê³¼ì—ì„œëŠ” ì›ë³¸ ì±„ë„ ì´ë¦„ì´ ë°°ì§€ë¡œ ë¶™ì–´ ìˆìŠµë‹ˆë‹¤.
					badge_tag = a_item.find_parent('div').find_parent('div').select_one('span.badge.badge-success')
					if badge_tag:
						gallery_id_for_output = badge_tag.get_text(strip=True)
					else:
						gallery_id_for_output = "Unknown Channel"
				
				
				# ëœë¤ ë”œë ˆì´
				time.sleep(random.uniform(1.5, 3.5)) 
				
				# ê²Œì‹œë¬¼ ë³¸ë¬¸ ìš”ì²­
				article_contents = ""
				comments_formatted = "" # ëŒ“ê¸€ ì €ì¥ ë³€ìˆ˜ ì´ˆê¸°í™”

				try:
					print(f" 	 -> ê²Œì‹œë¬¼ ë³¸ë¬¸ ìš”ì²­: {title_raw[:20]}... (ID: {post_id}, ì±„ë„: {gallery_id_for_output})")
					driver.get(post_full_url) 
					
					# ë³¸ë¬¸ ë¡œë”© ì™„ë£Œ ëŒ€ê¸° (div.article-content)
					WebDriverWait(driver, 10).until(
						EC.presence_of_element_located((By.CSS_SELECTOR, 'div.article-content'))
					)
					
					article_soup = BeautifulSoup(driver.page_source, 'lxml')

					# 1. ë³¸ë¬¸ ì¶”ì¶œ
					article_contents_tag = article_soup.find('div', class_='article-content')
					if article_contents_tag:
						article_contents = article_contents_tag.get_text('\n', strip=True)
					
					# 2. ëŒ“ê¸€ ì¶”ì¶œ (ì‘ì„±ì ì œì™¸)
					comment_items = article_soup.select('div.comment-item')
					extracted_comments = []
					
					for c_item in comment_items:
						text_tag = c_item.select_one('div.message div.text pre')
						if text_tag:
							c_text = text_tag.get_text('\n', strip=True)
							if c_text:
								extracted_comments.append(c_text)
					
					# Geminiê°€ ì¸ì‹í•˜ê¸° ì¢‹ë„ë¡ ë‹¨ìˆœ ë²ˆí˜¸ ë§¤ê¸°ê¸° (ì˜ˆ: 1. ëŒ“ê¸€ë‚´ìš©)
					if extracted_comments:
						comments_formatted = " ||| ".join(extracted_comments)

				except TimeoutException:
					print(f" 	 -> ê²Œì‹œë¬¼ ë³¸ë¬¸ ë¡œë“œ ì‹œê°„ ì´ˆê³¼ ({post_full_url}). ë³¸ë¬¸/ëŒ“ê¸€ ìˆ˜ì§‘ ê±´ë„ˆëœë‹ˆë‹¤.")
					continue 
				except WebDriverException as e:
					print(f" 	 -> ê²Œì‹œë¬¼ ìš”ì²­ ì¤‘ WebDriver ì˜¤ë¥˜ ({post_full_url}): {e}")
					continue
				
				# ----------------------
				# 3ë‹¨ê³„: ë°ì´í„° í´ë¦¬ë‹ ë° ì €ì¥
				# ----------------------
				
				# URL ì œê±° (ì œëª©, ë³¸ë¬¸)
				pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$\-@\.&+:/?=]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
				repl = ''
				title_clean = re.sub(pattern=pattern, repl=repl, string=title_raw).strip()
				article_contents_clean = re.sub(pattern=pattern, repl=repl, string=article_contents).strip()
				
				# ìµœì†Œí•œì˜ ë‚´ìš©(ë³¸ë¬¸)ì´ ìˆì–´ì•¼ ì €ì¥í•©ë‹ˆë‹¤.
				if article_contents_clean:
					data_list.append({
						'Site': 'ARCALIVE',
						'PostID': post_id,
						'Title': title_clean,
						'Content': article_contents_clean,
						'Comments': comments_formatted, # í¬ë§·íŒ…ëœ ëŒ“ê¸€ ë¬¸ìì—´ ì €ì¥
						'GalleryID': gallery_id_for_output, 
						'PostURL': post_full_url
					})

	finally:
		if driver:
			driver.quit() # ì‘ì—… ì™„ë£Œ í›„ ë“œë¼ì´ë²„ ì¢…ë£Œ
			print("--- WebDriver ì¢…ë£Œ ---")

	# ----------------------
	# 4ë‹¨ê³„: ë¦¬ìŠ¤íŠ¸ë¥¼ ìµœì¢… DataFrameìœ¼ë¡œ ë³€í™˜ ë° ì¤‘ë³µ ì œê±°
	# ----------------------
	df = pd.DataFrame(data_list)

	# PostIDë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ í–‰ ì œê±° 
	if not df.empty:
		df = df.drop_duplicates(subset=['GalleryID', 'PostID'], keep='first')
		print(f"\n--- í¬ë¡¤ë§ ì™„ë£Œ ë° ì¤‘ë³µ ì œê±° ---")
		print(f"ì´ ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ ìˆ˜ (ì›ë³¸): {len(data_list)}ê°œ")
		print(f"ì¤‘ë³µ ì œê±° í›„ ìµœì¢… ê²Œì‹œë¬¼ ìˆ˜: {len(df)}ê°œ")
			
	return df