import time
import random
import re
import pandas as pd
import urllib.parse

# Selenium ê´€ë ¨ Import
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException, UnexpectedAlertPresentException

# BeautifulSoup Import
from bs4 import BeautifulSoup

# -----------------------------------------------------------
# ì„¤ì • ë° ìƒìˆ˜ ì •ì˜
# -----------------------------------------------------------

# robots.txtì— ëª…ì‹œëœ í¬ë¡¤ë§ ê¸ˆì§€(Disallow) ê°¤ëŸ¬ë¦¬ ID ëª©ë¡
DISALLOWED_IDS = {
    '47', 'singo', 'stock_new', 'cat', 'dog', 'baseball_new8', 'm_entertainer1',
    'stock_new2', 'ib_new', 'd_fighter_new1', 'produce48', 'sportsseoul', 
    'metakr', 'salgoonews', 'rezero'
}

# User-Agent ëª©ë¡ ì •ì˜
USER_AGENT_LIST = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 14_2_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
]

def get_driver():
    """Selenium WebDriver ì„¤ì •ì„ ì´ˆê¸°í™”í•˜ê³  ë“œë¼ì´ë²„ ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    from selenium.webdriver.chrome.service import Service
    from webdriver_manager.chrome import ChromeDriverManager
    
    options = webdriver.ChromeOptions()
    options.add_argument('headless')
    options.add_argument('window-size=1920x1080')
    options.add_argument('disable-gpu')
    options.add_argument('log-level=3')
    options.add_argument('disable-infobars')
    options.add_argument('--disable-extensions')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    
    # [í•µì‹¬ ìˆ˜ì • 1] í˜ì´ì§€ ë¡œë“œ ì „ëµ: 'eager'
    # normal: ëª¨ë“  ë¦¬ì†ŒìŠ¤(ì´ë¯¸ì§€, CSS, ê´‘ê³  ë“±)ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸° (ê°€ì¥ ëŠë¦¬ê³  ë¬´ê±°ì›€)
    # eager: DOMContentLoaded ì´ë²¤íŠ¸ê¹Œì§€ë§Œ ëŒ€ê¸° (ì´ë¯¸ì§€ ë¡œë”© ì•ˆ ê¸°ë‹¤ë¦¼ -> í›¨ì”¬ ë¹ ë¥´ê³  ê°€ë²¼ì›€)
    options.page_load_strategy = 'eager'

    # [í•µì‹¬ ìˆ˜ì • 2] ê°•ë ¥í•œ ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨ ì„¤ì • (ì´ë¯¸ì§€, JS íŒì—… ë“± ì°¨ë‹¨)
    prefs = {
        "profile.managed_default_content_settings.images": 2,  # ì´ë¯¸ì§€ ë¡œë”© ì°¨ë‹¨ (2=Block)
        "profile.default_content_setting_values.notifications": 2, # ì•Œë¦¼ ì°¨ë‹¨
        "profile.managed_default_content_settings.stylesheets": 2, # CSS ì¼ë¶€ ì°¨ë‹¨ (ë¸Œë¼ìš°ì €ì— ë”°ë¼ ì•ˆ ë¨¹í ìˆ˜ ìˆìŒ)
        "profile.managed_default_content_settings.cookies": 1,
        "profile.managed_default_content_settings.javascript": 1, # JSëŠ” ì¼œì•¼ í•¨ (1=Allow)
        "profile.managed_default_content_settings.plugins": 1,
        "profile.managed_default_content_settings.popups": 2,
        "profile.managed_default_content_settings.geolocation": 2,
        "profile.managed_default_content_settings.media_stream": 2,
    }
    options.add_experimental_option("prefs", prefs)

    user_agent = random.choice(USER_AGENT_LIST)
    options.add_argument(f'user-agent={user_agent}')
    
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    
    try:
        # [ìˆ˜ì • ì „] ìë™ ì„¤ì¹˜ (ì´ ë¶€ë¶„ì„ ì§€ìš°ê±°ë‚˜ ì£¼ì„ ì²˜ë¦¬)
        # service = Service(ChromeDriverManager().install())
        
        # [ìˆ˜ì • í›„] ê³ ì • ê²½ë¡œ ì§€ì •
        # ë„ì»¤/ë¦¬ëˆ…ìŠ¤ í™˜ê²½ì—ì„œ ì„¤ì¹˜ëœ ë“œë¼ì´ë²„ ê²½ë¡œ (ë³´í†µ /usr/bin/chromedriver)
        service = Service(executable_path='/usr/bin/chromedriver')
        
        driver = webdriver.Chrome(service=service, options=options)
        driver.set_page_load_timeout(30)
        print("[DEBUG] DC WebDriver ì´ˆê¸°í™” ì„±ê³µ")
        return driver
    except Exception as e:
        print(f"âŒ WebDriver ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None

def extract_comments(soup):
    """
    BeautifulSoup ê°ì²´ì—ì„œ ëŒ“ê¸€ì„ ì¶”ì¶œí•˜ì—¬ êµ¬ë¶„ìë¡œ ì—°ê²°ëœ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    êµ¬ì¡°: <ul class="cmt_list"> -> <li class="ub-content"> -> <p class="usertxt">
    * ëª¨ë¸ í•™ìŠµ/í•„í„°ë§ì„ ìœ„í•´ ë²ˆí˜¸ ì—†ì´ ' ||| ' êµ¬ë¶„ìë§Œ ì‚¬ìš©í•˜ì—¬ ì—°ê²°í•©ë‹ˆë‹¤.
    """
    comments_formatted = ""
    extracted_comments = []
    
    # ëŒ“ê¸€ ë¦¬ìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
    cmt_list = soup.select('ul.cmt_list li.ub-content')
    
    for li in cmt_list:
        # ì‚­ì œëœ ëŒ“ê¸€ ë“±ì€ ì œì™¸í•˜ê³  ì‹¤ì œ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ì¶œ
        txt_box = li.select_one('div.cmt_txtbox p.usertxt')
        
        if txt_box:
            c_text = txt_box.get_text('\n', strip=True)
            if c_text:
                extracted_comments.append(c_text)
                
    # ê²°ê³¼ í¬ë§·íŒ… (ë‚´ìš© ||| ë‚´ìš©)
    if extracted_comments:
        comments_formatted = " ||| ".join(extracted_comments)
        
    return comments_formatted

# -----------------------------------------------------------
# 1. ì¼ë°˜ ê°¤ëŸ¬ë¦¬ í¬ë¡¤ë§ í•¨ìˆ˜ (Selenium ì ìš©)
# -----------------------------------------------------------
def get_regular_post_data(gallery_id: str, gallery_type: str = "minor", search_keyword: str = "", search_option: int = 0, start_page: int = 1, end_page: int = 1) -> pd.DataFrame:
    
    data_list = []
    BASE_URL = "https://gall.dcinside.com"

    # ë¡œë´‡ ë°°ì œ í™•ì¸
    if gallery_id in DISALLOWED_IDS:
        print(f"\nğŸš¨ ê²½ê³ : ê°¤ëŸ¬ë¦¬ ID '{gallery_id}'ëŠ” í¬ë¡¤ë§ ê¸ˆì§€ ëŒ€ìƒì…ë‹ˆë‹¤.")
        
        data_list.append({
            'Site': 'DCINSIDE',
            'PostID': 'robots.txt disallow',
            'Title': 'robots.txt disallow',
            'Content': f"\nğŸš¨ ê²½ê³ : ê°¤ëŸ¬ë¦¬ ID '{gallery_id}'ëŠ” í¬ë¡¤ë§ ê¸ˆì§€ ëŒ€ìƒì…ë‹ˆë‹¤.",
            'Comments': 'robots.txt disallow',
            'GalleryID': 'robots.txt disallow',
            'PostURL': 'robots.txt disallow'
        })
        return pd.DataFrame(data_list)

    # ê°¤ëŸ¬ë¦¬ íƒ€ì…ì— ë”°ë¥¸ URL ì„¤ì •
    if gallery_type == "minor":
        board_path = "/mgallery/board/lists/"
    elif gallery_type == "major":
        board_path = "/board/lists/"
    elif gallery_type == "mini":
        board_path = "/mini/board/lists/"
    else:
        print("ì˜ëª»ëœ ê°¤ëŸ¬ë¦¬ íƒ€ì…ì…ë‹ˆë‹¤.")
        return pd.DataFrame(data_list)

    driver = get_driver()
    if not driver:
        return pd.DataFrame(data_list)

    try:
        for i in range(int(start_page), int(end_page) + 1):
            
            # --- 1ë‹¨ê³„: ëª©ë¡ í˜ì´ì§€ URL êµ¬ì„± ---
            params = {'id': gallery_id, 'page': i}
            
            if search_keyword:
                params['search_pos'] = ''
                if search_option == 0: params['s_type'] = 'search_subject_memo'
                elif search_option == 1: params['s_type'] = 'search_subject'
                elif search_option == 2: params['s_type'] = 'search_memo'
                
                params['s_keyword'] = search_keyword
            
            full_list_url = f"{BASE_URL}{board_path}?{urllib.parse.urlencode(params)}"
            print(f"--- [DC ì¼ë°˜] ëª©ë¡ í˜ì´ì§€ {i} ì§„ì…. ê°¤ëŸ¬ë¦¬: {gallery_id}, ê²€ìƒ‰ì–´: {search_keyword}, URL: {full_list_url} ---")
            
            try:
                driver.get(full_list_url)
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, 'tbody tr.ub-content'))
                )
            except (TimeoutException, UnexpectedAlertPresentException):
                print(f"[DC ì¼ë°˜] ëª©ë¡ í˜ì´ì§€ {i} ë¡œë”© ì‹¤íŒ¨ ë˜ëŠ” ì•Œë¦¼ì°½ ë°œìƒ. ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™.")
                continue

            # BS4ë¡œ ëª©ë¡ íŒŒì‹±
            soup = BeautifulSoup(driver.page_source, 'lxml')
            article_rows = soup.select('tbody tr.ub-content')
            
            valid_rows = []
            for row in article_rows:
                # 1. data-type ê¸°ë°˜ ê³µì§€ í•„í„°ë§
                data_type = row.get('data-type')
                if data_type and 'icon_notice' in data_type: continue

                # 2. ì‘ì„±ì(ìš´ì˜ì) í•„í„°ë§
                writer_td = row.select_one('td.gall_writer')
                if writer_td:
                    if writer_td.get('user_name') == 'ìš´ì˜ì': continue
                    if writer_td.get_text(strip=True) == 'ìš´ì˜ì': continue

                # 3. ë§ë¨¸ë¦¬(ì´ìŠˆ, ê³µì§€ ë“±) í•„í„°ë§
                subject_td = row.select_one('td.gall_subject')
                if subject_td:
                    subject_txt = subject_td.get_text(strip=True)
                    if subject_txt == 'ê³µì§€': continue

                valid_rows.append(row)

            if not valid_rows:
                print(f"[DC ì¼ë°˜] í˜ì´ì§€ {i}ì— ìˆ˜ì§‘ ê°€ëŠ¥í•œ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                continue

            print(f"-> [DC ì¼ë°˜] í˜ì´ì§€ {i}ì—ì„œ {len(valid_rows)}ê°œì˜ ê²Œì‹œë¬¼ ë°œê²¬.")

            # --- 2ë‹¨ê³„: ê°œë³„ ê²Œì‹œë¬¼ ìˆœíšŒ ---
            for row in valid_rows:
                title_tag = row.select_one('a[href*="&no="]')
                if not title_tag: continue
                
                title_raw = title_tag.get_text(strip=True)
                relative_url = title_tag['href']
                
                post_id_match = re.search(r'&no=(\d+)', relative_url)
                post_id = post_id_match.group(1) if post_id_match else None
                
                if not post_id: continue
                
                if relative_url.startswith('http'):
                    post_full_url = relative_url
                else:
                    post_full_url = BASE_URL + relative_url

                # ëœë¤ ë”œë ˆì´
                time.sleep(random.uniform(1.5, 3.5))

                # --- 3ë‹¨ê³„: ë³¸ë¬¸ ë° ëŒ“ê¸€ ìˆ˜ì§‘ ---
                try:
                    print(f"   -> [DC ì¼ë°˜] ê²Œì‹œë¬¼ ì ‘ì†: {title_raw[:20]}... (ID: {post_id}, ê°¤ëŸ¬ë¦¬: {gallery_id})")
                    driver.get(post_full_url)
                    
                    # 1. ê°€ì¥ ì¤‘ìš”í•œ ë³¸ë¬¸ì´ ëœ° ë•Œê¹Œì§€ í™•ì‹¤íˆ ê¸°ë‹¤ë¦¼ (í•„ìˆ˜)
                    WebDriverWait(driver, 5).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, 'div.write_div'))
                    )

                    # 2. ëŒ“ê¸€ ì˜ì—­ ë¡œë”© ëŒ€ê¸° (ì„ íƒ ì‚¬í•­ - íƒ€ì„ì•„ì›ƒ ì˜ˆì™¸ ì²˜ë¦¬ í•„ìˆ˜)
                    try:
                        WebDriverWait(driver, 1).until(
                            EC.presence_of_element_located((By.CSS_SELECTOR, 'div.comment_wrap'))
                        )
                    except TimeoutException:
                        # ëŒ“ê¸€ ì˜ì—­ì„ ëª» ì°¾ì•„ë„(ë„¤íŠ¸ì›Œí¬ ëŠë¦¼ or êµ¬ì¡° ë³€ê²½ ë“±) ë³¸ë¬¸ì€ ìˆ˜ì§‘í•´ì•¼ í•˜ë¯€ë¡œ ê·¸ëƒ¥ ë„˜ì–´ê°
                        print("[DC ì¼ë°˜] ëŒ“ê¸€ ë¡œë”© ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    
                    post_soup = BeautifulSoup(driver.page_source, 'lxml')
                    
                    # A. ë³¸ë¬¸ ì¶”ì¶œ
                    content_div = post_soup.find('div', class_='write_div')
                    content_text = content_div.get_text('\n', strip=True) if content_div else ""
                    
                    # B. ëŒ“ê¸€ ì¶”ì¶œ
                    comments_text = extract_comments(post_soup)
                    
                    # C. ë°ì´í„° í´ë¦¬ë‹
                    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$\-@\.&+:/?=]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
                    title_clean = re.sub(url_pattern, '', title_raw).strip()
                    content_clean = re.sub(url_pattern, '', content_text).strip()
                    content_clean = content_clean.replace('- dc official App', '').replace('- dc App', '').strip()
                    
                    if content_clean:
                        data_list.append({
                            'Site': 'DCINSIDE',
                            'PostID': post_id,
                            'Title': title_clean,
                            'Content': content_clean,
                            'Comments': comments_text,
                            'GalleryID': gallery_id,
                            'PostURL': post_full_url
                        })

                except Exception as e:
                    print(f"   -> [DC ì¼ë°˜] ìƒì„¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                    continue

    finally:
        driver.quit()
        print("--- WebDriver ì¢…ë£Œ ---")
    
    # ê²°ê³¼ DF ìƒì„± ë° ì¤‘ë³µ ì œê±°
    df = pd.DataFrame(data_list)
    if not df.empty:
        df = df.drop_duplicates(subset=['GalleryID', 'PostID'], keep='first')
        print(f"\n--- [DC ì¼ë°˜] í¬ë¡¤ë§ ì™„ë£Œ ë° ì¤‘ë³µ ì œê±° ---")
        print(f"ì´ ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ ìˆ˜ (ì›ë³¸): {len(data_list)}ê°œ")
        print(f"ì¤‘ë³µ ì œê±° í›„ ìµœì¢… ê²Œì‹œë¬¼ ìˆ˜: {len(df)}ê°œ")
        
    return df


# -----------------------------------------------------------
# 2. í†µí•© ê²€ìƒ‰ í¬ë¡¤ë§ í•¨ìˆ˜ (Selenium ì ìš©)
# -----------------------------------------------------------
def get_integrated_search_data(search_keyword: str, sort_type: str = "latest", start_page: int = 1, end_page: int = 1) -> pd.DataFrame:
    
    data_list = []
    SEARCH_BASE_URL = "https://search.dcinside.com/post/"
    
    # ê²€ìƒ‰ì–´ ì¸ì½”ë”©
    encoded_keyword = urllib.parse.quote(search_keyword).replace('%', '.')
    sort_path = "sort/accuracy/" if sort_type == "accuracy" else ""
    
    driver = get_driver()
    if not driver:
        return pd.DataFrame(data_list)

    try:
        for i in range(int(start_page), int(end_page) + 1):
            
            # ê²€ìƒ‰ URL êµ¬ì„±
            full_search_url = f"{SEARCH_BASE_URL}p/{i}/{sort_path}q/{encoded_keyword}"
            print(f"--- [DC í†µí•©] ê²€ìƒ‰ í˜ì´ì§€ {i} ì§„ì…. ê²€ìƒ‰ì–´: {search_keyword} , URL: {full_search_url} ---")
            
            try:
                driver.get(full_search_url)
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, 'ul.sch_result_list'))
                )
            except TimeoutException:
                print(f"[DC í†µí•©] ê²€ìƒ‰ í˜ì´ì§€ {i} ë¡œë”© ì‹¤íŒ¨. ì¢…ë£Œ.")
                break
                
            soup = BeautifulSoup(driver.page_source, 'lxml')
            result_items = soup.select('ul.sch_result_list li')
            
            if not result_items:
                print("[DC í†µí•©] ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                break
                
            # ê²°ê³¼ ì•„ì´í…œ ìˆœíšŒ
            for item in result_items:
                link_tag = item.select_one('a.tit_txt')
                if not link_tag: continue
                
                post_url = link_tag.get('href')
                title_raw = link_tag.get_text(strip=True)
                
                # ê°¤ëŸ¬ë¦¬ ì •ë³´ ì¶”ì¶œ
                meta_tag = item.select_one('p.link_dsc_txt.dsc_sub a.sub_txt')
                gallery_name = meta_tag.get_text(strip=True) if meta_tag else "Unknown"
                
                # ê°¤ëŸ¬ë¦¬ ID ì¶”ì¶œ (URL íŒŒì‹±)
                gallery_id = "N/A"
                if meta_tag and 'id=' in meta_tag.get('href', ''):
                    gallery_id = meta_tag['href'].split('id=')[1].split('&')[0]
                
                if gallery_id in DISALLOWED_IDS:
                    continue
                    
                if 'no=' in post_url:
                    post_id = re.search(r'no=(\d+)', post_url).group(1)
                else:
                    continue

                # ìƒì„¸ í˜ì´ì§€ ì§„ì…
                time.sleep(random.uniform(1.5, 3.5))
                
                try:
                    print(f"   -> [DC í†µí•©] ê²€ìƒ‰ ê²Œì‹œë¬¼ ì ‘ì†: {title_raw[:20]}... (ID: {post_id}, ê°¤ëŸ¬ë¦¬: {gallery_name})")
                    driver.get(post_url)
                    
                    # 1. ê°€ì¥ ì¤‘ìš”í•œ ë³¸ë¬¸ì´ ëœ° ë•Œê¹Œì§€ í™•ì‹¤íˆ ê¸°ë‹¤ë¦¼ (í•„ìˆ˜)
                    WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, 'div.write_div'))
                    )

                    # 2. ëŒ“ê¸€ ì˜ì—­ ë¡œë”© ëŒ€ê¸° (ì„ íƒ ì‚¬í•­ - íƒ€ì„ì•„ì›ƒ ì˜ˆì™¸ ì²˜ë¦¬ í•„ìˆ˜)
                    try:
                        WebDriverWait(driver, 1).until(
                            EC.presence_of_element_located((By.CSS_SELECTOR, 'div.comment_wrap'))
                        )
                    except TimeoutException:
                        # ëŒ“ê¸€ ì˜ì—­ì„ ëª» ì°¾ì•„ë„(ë„¤íŠ¸ì›Œí¬ ëŠë¦¼ or êµ¬ì¡° ë³€ê²½ ë“±) ë³¸ë¬¸ì€ ìˆ˜ì§‘í•´ì•¼ í•˜ë¯€ë¡œ ê·¸ëƒ¥ ë„˜ì–´ê°
                        print("[DC í†µí•©] ëŒ“ê¸€ ë¡œë”© ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    
                    post_soup = BeautifulSoup(driver.page_source, 'lxml')
                    
                    content_div = post_soup.find('div', class_='write_div')
                    content_text = content_div.get_text('\n', strip=True) if content_div else ""
                    
                    comments_text = extract_comments(post_soup)
                    
                    # í´ë¦¬ë‹
                    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$\-@\.&+:/?=]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
                    title_clean = re.sub(url_pattern, '', title_raw).strip()
                    content_clean = re.sub(url_pattern, '', content_text).strip()
                    content_clean = content_clean.replace('- dc official App', '').replace('- dc App', '').strip()
                    
                    data_list.append({
                        'Site': 'DCINSIDE',
                        'PostID': post_id,
                        'Title': title_clean,
                        'Content': content_clean,
                        'Comments': comments_text,
                        'GalleryID': gallery_name,
                        'PostURL': post_url
                    })
                    
                except Exception as e:
                    print(f"   -> [DC í†µí•©] ìƒì„¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                    continue

    finally:
        driver.quit()
        print("--- ê²€ìƒ‰ WebDriver ì¢…ë£Œ ---")
        
    df = pd.DataFrame(data_list)
    if not df.empty:
        df = df.drop_duplicates(subset=['GalleryID', 'PostID'], keep='first')
        print(f"\n--- [DC í†µí•©] í¬ë¡¤ë§ ì™„ë£Œ ë° ì¤‘ë³µ ì œê±° ---")
        print(f"ì´ ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ ìˆ˜ (ì›ë³¸): {len(data_list)}ê°œ")
        print(f"ì¤‘ë³µ ì œê±° í›„ ìµœì¢… ê²Œì‹œë¬¼ ìˆ˜: {len(df)}ê°œ")
        
    return df

# -----------------------------------------------------------
# 3. [NEW] DC í†µí•© ì¸í„°í˜ì´ìŠ¤ (Wrapper)
# -----------------------------------------------------------
def search_dc_inside(search_keyword: str, start_page: int = 1, end_page: int = 1, **kwargs) -> pd.DataFrame:
    """
    DC ì¸ì‚¬ì´ë“œ ë‚´ì˜ ëª¨ë“  ê²€ìƒ‰ ìš”ì²­(í†µí•© ê²€ìƒ‰ ë° ê°¤ëŸ¬ë¦¬ ê²€ìƒ‰)ì„ ì²˜ë¦¬í•˜ëŠ” ë‹¨ì¼ ì§„ì…ì ì…ë‹ˆë‹¤.
    **kwargsì— 'gallery_id'ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì¼ë°˜ ê°¤ëŸ¬ë¦¬ ê²€ìƒ‰ìœ¼ë¡œ,
    ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ í†µí•© ê²€ìƒ‰ìœ¼ë¡œ ë¶„ê¸°í•©ë‹ˆë‹¤.
    
    Args:
        search_keyword (str): ê²€ìƒ‰ì–´
        start_page (int): ì‹œì‘ í˜ì´ì§€
        end_page (int): ì¢…ë£Œ í˜ì´ì§€
        **kwargs:
            - gallery_id (str): ê°¤ëŸ¬ë¦¬ ID (ì¡´ì¬ ì‹œ ê°¤ëŸ¬ë¦¬ ê²€ìƒ‰)
            - gallery_type (str): ê°¤ëŸ¬ë¦¬ íƒ€ì… (ê¸°ë³¸ 'minor')
            - search_option (int): ê²€ìƒ‰ ì˜µì…˜ (ê¸°ë³¸ 0)
            - sort_type (str): í†µí•© ê²€ìƒ‰ ì •ë ¬ ë°©ì‹ (ê¸°ë³¸ 'latest')
    """
    
    # 1. gallery_idê°€ ì¸ìì— ìˆìœ¼ë©´ -> íŠ¹ì • ê°¤ëŸ¬ë¦¬ ê²€ìƒ‰
    if 'gallery_id' in kwargs and kwargs['gallery_id']:
        gallery_id = kwargs['gallery_id']
        gallery_type = kwargs.get('gallery_type', 'minor')
        search_option = kwargs.get('search_option', 0)
        
        print(f"ğŸš€ [DC Wrapper] '{gallery_id}' ê°¤ëŸ¬ë¦¬ ê²€ìƒ‰ ëª¨ë“œë¡œ ì§„ì…")
        return get_regular_post_data(
            gallery_id=gallery_id,
            gallery_type=gallery_type,
            search_keyword=search_keyword,
            search_option=search_option,
            start_page=start_page,
            end_page=end_page
        )
        
    # 2. gallery_idê°€ ì—†ìœ¼ë©´ -> DC ì „ì²´ í†µí•© ê²€ìƒ‰
    else:
        sort_type = kwargs.get('sort_type', 'latest')
        print(f"ğŸš€ [DC Wrapper] í†µí•© ê²€ìƒ‰ ëª¨ë“œë¡œ ì§„ì…")
        return get_integrated_search_data(
            search_keyword=search_keyword,
            sort_type=sort_type,
            start_page=start_page,
            end_page=end_page
        )