import requests
from bs4 import BeautifulSoup
import time    # ëœë¤ ë”œë ˆì´ì‹œ
import random  # ëœë¤ ë”œë ˆì´ì‹œ
import re  # ì •ê·œ í‘œí˜„ì‹
import pandas as pd # Pandas df ì‚¬ìš©
import urllib.parse # URL ì¸ì½”ë”©ìš©

# User-Agent ëª©ë¡ ì •ì˜(ëœë¤ì„ íƒ)
USER_AGENT_LIST = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 14_2_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
]

# robots.txtì— ëª…ì‹œëœ í¬ë¡¤ë§ ê¸ˆì§€(Disallow) ê°¤ëŸ¬ë¦¬ ID ëª©ë¡ ì •ì˜
# ì´ ëª©ë¡ì€ '/board/lists/?id=' ë˜ëŠ” '/mgallery/board/lists/?id='ë¡œ ê¸ˆì§€ëœ IDì…ë‹ˆë‹¤.
DISALLOWED_IDS = {
    '47', 'singo', 'stock_new', 'cat', 'dog', 'baseball_new8', 'm_entertainer1',
    'stock_new2', 'ib_new', 'd_fighter_new1', 'produce48', 'sportsseoul', 
    'metakr', 'salgoonews', 'rezero'
}

def get_regular_post_data(gallery_id: str, gallery_type: str = "minor", search_keyword: str = "", search_option: int = 0, start_page: int = 1, end_page: int = 3) -> pd.DataFrame:
    """
    PC ê°¤ëŸ¬ë¦¬ í˜ì´ì§€ì—ì„œ ê²Œì‹œë¬¼ì˜ ì œëª©ê³¼ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì—¬ DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    
    data_list = []

    BASE_URL = "https://gall.dcinside.com"

    # robots.txt disallow í•„í„°ë§
    if gallery_id in DISALLOWED_IDS:
        print(f"\nğŸš¨ ê²½ê³ : ê°¤ëŸ¬ë¦¬ ID '{gallery_id}'ëŠ” robots.txtì— ì˜í•´ í¬ë¡¤ë§ì´ ê¸ˆì§€ëœ IDì…ë‹ˆë‹¤. ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return pd.DataFrame(data_list)

    # ê°¤ëŸ¬ë¦¬ ì¢…ë¥˜ë³„ ì£¼ì†Œ ì„¤ì •
    if gallery_type == "minor":
        gallery_type_url = "/mgallery/board/lists"
    elif gallery_type == "major":
        gallery_type_url = "/board/lists"
    elif gallery_type == "mini":
        gallery_type_url = "/mini/board/lists"
    else:
        print("gallery_type ì¸ìê°€ ì˜ëª» ë˜ì—ˆìŠµë‹ˆë‹¤. ë¹ˆ dfë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
        return pd.DataFrame(data_list)
    
    for i in range(start_page, end_page + 1):
        
        # ----------------------
        # 1ë‹¨ê³„: ëª©ë¡ í˜ì´ì§€ ìš”ì²­ ë° íŒŒì‹±
        # ----------------------
        
        params = {'id': gallery_id, 'page': i}

        # ê²€ìƒ‰ ì£¼ì†Œ ì¡°ë¦½ ì‹œ í•„ìš”í•œ íŒŒë¼ë¯¸í„° ì •ì˜
        # ex) https://gall.dcinside.com/mgallery/board/lists/?id={GalleryID}&s_type={search_option}&s_keyword={search_keyword}
        if search_keyword:
            # PC ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ì‚¬ìš©
            params['search_pos'] = ''

            # ê²€ìƒ‰ ì˜µì…˜ ë³„ ì£¼ì†Œ ì„¤ì •
            if search_option == 0:
                params['s_type'] = 'search_subject_memo'
            elif search_option == 1:
                params['s_type'] = 'search_subject'
            elif search_option == 2:
                params['s_type'] = 'search_memo'
            else:
                print("search_option ì¸ìˆ˜ê°€ ì˜ëª» ë˜ì—ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ì¸ 0(ì œëª©, ë‚´ìš© ê²€ìƒ‰)ìœ¼ë¡œ ì„¤ì •ë©ë‹ˆë‹¤.")
                params['s_type'] = 'search_subject_memo'
                
            params['s_keyword'] = search_keyword

        # User-Agent ì„¤ì •
        user_agent = random.choice(USER_AGENT_LIST)
        headers = {'User-Agent': user_agent}

        # try-except
        try:
            print(f"--- ê°¤ëŸ¬ë¦¬ ëª©ë¡ í˜ì´ì§€ {i} ìš”ì²­ ì¤‘ ---")
            full_url = BASE_URL + gallery_type_url
            response = requests.get(full_url, params=params, headers=headers, timeout=10)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"ëª©ë¡ í˜ì´ì§€ {i} ìš”ì²­ ì‹¤íŒ¨: {e}. ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
            time.sleep(random.uniform(2, 4))
            continue

        # lxml íŒŒì„œ ì‚¬ìš©(HTML ëŒ€ì‹ )
        soup = BeautifulSoup(response.content, 'lxml')
        
        # ê¸€ ëª©ë¡ êµ¬ì¡°: <tbody> ë‚´ì˜ <tr>
        article_list = soup.find('tbody').find_all('tr', {'data-type': ['icon_pic', 'icon_txt']})
        
        # ê¸°ë³¸ ê³µì§€, ê´‘ê³ ê¸€ í•„í„°ë§
        # ì¼ë°˜ì ìœ¼ë¡œ ì—†ì–´ë„ ë¬´ê´€í•˜ì§€ë§Œ ê³µë°± ê²€ìƒ‰ì‹œ í¬í•¨ë¨
        filtered_articles = []
        for tr_item in article_list:
            writer_tag = tr_item.find('td', class_='gall_writer')
            is_operator_post = writer_tag and writer_tag.get('user_name') == 'ìš´ì˜ì'
            is_notice = tr_item.get('data-type') == 'icon_notice'
            
            if not is_operator_post and not is_notice:
                filtered_articles.append(tr_item)
                
        if not filtered_articles:
             print(f"í˜ì´ì§€ {i}ì—ì„œ ìœ íš¨í•œ ì¼ë°˜ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.")
             break 


        # ----------------------
        # 2ë‹¨ê³„: ê°œë³„ ê²Œì‹œë¬¼ ì ‘ê·¼ ë° ë‚´ìš© ì¶”ì¶œ 
        # ----------------------
        for tr_item in filtered_articles:
            
            title_tag = tr_item.find('a', href=True)
            if not title_tag: continue

            title_raw = title_tag.text.strip()
            relative_url = title_tag['href']

            # ê²Œì‹œê¸€ ID ì €ì¥
            post_id_match = re.search(r'&no=(\d+)', relative_url)
            post_id = post_id_match.group(1) if post_id_match else None

            # ê²Œì‹œê¸€ ID ì˜¤ë¥˜ ì‹œ ê±´ë„ˆë›°ê¸°
            if not post_id:
                print(f"    -> ì˜¤ë¥˜: ê²Œì‹œë¬¼ ë²ˆí˜¸ ì¶”ì¶œ ì‹¤íŒ¨ ({BASE_URL + relative_url}). ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
            
            # href ì ˆëŒ€ ê²½ë¡œ/ìƒëŒ€ ê²½ë¡œ ëª¨ë‘ ëŒ€ì‘ (ì—†ì–´ë„ ì†”ì§íˆ ë¬¸ì œ ì—†ì„ë“¯?)
            if relative_url.startswith('http'):
                full_url = relative_url
            else:
                full_url = BASE_URL + relative_url

            # ëœë¤ ë”œë ˆì´
            time.sleep(random.uniform(3, 5))
            
            # ê²Œì‹œë¬¼ ë³¸ë¬¸ ìš”ì²­
            try:
                print(f"   -> ê²Œì‹œë¬¼ ìš”ì²­: {title_raw[:20]}...")
                article_user_agent = random.choice(USER_AGENT_LIST)
                article_headers = {'User-Agent': article_user_agent}
                article_response = requests.get(full_url, headers=article_headers, timeout=10)
                article_response.raise_for_status()
            except requests.exceptions.RequestException as e:
                print(f"   -> ê²Œì‹œë¬¼ ìš”ì²­ ì‹¤íŒ¨ ({full_url}): {e}")
                continue
            
            article_soup = BeautifulSoup(article_response.content, 'lxml') # lxml ì‚¬ìš©

            # ë³¸ë¬¸ ì¶”ì¶œ í´ë˜ìŠ¤: 'write_div'
            article_contents_tag = article_soup.find('div', class_='write_div')
            article_contents = ""
            if article_contents_tag:
                # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                article_contents = article_contents_tag.get_text(strip=True)
            
            # ----------------------
            # 3ë‹¨ê³„: ë°ì´í„° í´ë¦¬ë‹ ë° ì €ì¥
            # ----------------------
            
            # ì œëª©ê³¼ ê²Œì‹œê¸€ì—ì„œ url ì œê±°
            pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$\-@\.&+:/?=]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
            repl = ''
            title_clean = re.sub(pattern=pattern, repl=repl, string=title_raw).strip()
            article_contents_clean = re.sub(pattern=pattern, repl=repl, string=article_contents).strip()
            
            # '- dc official App' ì œê±°
            article_contents_clean = article_contents_clean.replace('- dc official App', '').strip()
            
            
            if article_contents_clean:
                data_list.append({
                    'PostID': post_id,
                    'Title': title_clean,
                    'Content': article_contents_clean,
                    'Comments': None,
                    'GalleryID': gallery_id,
                    'PostURL': full_url
                })

    # ----------------------
    # 4ë‹¨ê³„: ë¦¬ìŠ¤íŠ¸ë¥¼ ìµœì¢… DataFrameìœ¼ë¡œ ë³€í™˜ ë° ì¤‘ë³µ ì œê±°
    # ----------------------
    df = pd.DataFrame(data_list)

    # PostIDë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ í–‰ ì œê±° (í˜ì´ì§€ê°€ ê²¹ì³ì„œ ì¬ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ ì œê±°)
    if not df.empty:
        df = df.drop_duplicates(subset=['GalleryID', 'PostID'], keep='first')
        print(f"\n--- í¬ë¡¤ë§ ì™„ë£Œ ë° ì¤‘ë³µ ì œê±° ---")
        print(f"ì´ ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ ìˆ˜: {len(data_list)}ê°œ")
        print(f"ì¤‘ë³µ ì œê±° í›„ ìµœì¢… ê²Œì‹œë¬¼ ìˆ˜: {len(df)}ê°œ")
             
    return df

def get_integrated_search_data(search_keyword: str, sort_type: str = "latest", start_page: int = 1, end_page: int = 3) -> pd.DataFrame:
    """
    DC Inside í†µí•© ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì—ì„œ ê²Œì‹œë¬¼ ë©”íƒ€ë°ì´í„°ì™€ ë³¸ë¬¸ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì—¬ DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    (ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ -> ê°œë³„ ê²Œì‹œë¬¼ ë³¸ë¬¸ ìš”ì²­ ê³¼ì •ì„ í¬í•¨í•©ë‹ˆë‹¤.)
    """
    
    data_list = []
    
    # í†µí•© ê²€ìƒ‰ ê¸°ë³¸ URL
    SEARCH_BASE_URL = "https://search.dcinside.com/post/"
    
    # 1. í‚¤ì›Œë“œ íŠ¹ìˆ˜ ì¸ì½”ë”©
    encoded_keyword = urllib.parse.quote(search_keyword) 
    dc_encoded_keyword = encoded_keyword.replace('%', '.')
    
    # 2. ì •ë ¬ íƒ€ì… ì„¤ì •
    sort_url = ""
    # 'accuracy' (ì •í™•ë„ ìˆœ)
    if sort_type == "accuracy":
        sort_url = "sort/accuracy/"
    # 'latest' (ìµœì‹  ìˆœ)ì€ URLì—ì„œ ìƒëµ

    for i in range(start_page, end_page + 1):
        
        # URL ê²½ë¡œ ì¡°ë¦½: /post/p/{page}/[sort/accuracy/]/q/{encoded_keyword}
        full_url = f"{SEARCH_BASE_URL}p/{i}/{sort_url}q/{dc_encoded_keyword}"
        
        # User-Agent ì„¤ì •
        user_agent = random.choice(USER_AGENT_LIST)
        headers = {'User-Agent': user_agent}

        try:
            print(f"--- í†µí•© ê²€ìƒ‰ í˜ì´ì§€ {i} ìš”ì²­ ì¤‘: '{search_keyword}' ({sort_type}) ---")
            
            # ìˆ˜ë™ìœ¼ë¡œ ì¡°ë¦½ëœ URLì„ ìš”ì²­í•©ë‹ˆë‹¤.
            response = requests.get(full_url, headers=headers, timeout=10) 
            response.raise_for_status()
            
            # ìš”ì²­ëœ URL í™•ì¸ìš© ì¶œë ¥ (í•„ìš”ì— ë”°ë¼ ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©)
            #print(f"ìš”ì²­ URL: {response.url}")

        except requests.exceptions.RequestException as e:
            print(f"í†µí•© ê²€ìƒ‰ í˜ì´ì§€ {i} ìš”ì²­ ì‹¤íŒ¨: {e}. ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
            time.sleep(random.uniform(2, 4))
            continue
            
        soup = BeautifulSoup(response.content, 'lxml')

        # ê²€ìƒ‰ ê²°ê³¼ ì»¨í…Œì´ë„ˆ (ul.sch_result_list)ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        result_container = soup.find('ul', class_='sch_result_list')
        
        # ì»¨í…Œì´ë„ˆê°€ ë°œê²¬ë˜ë©´ ê·¸ ì•ˆì˜ ëª¨ë“  <li> í•­ëª©ì„ ì°¾ìŠµë‹ˆë‹¤.
        if result_container:
            result_list = result_container.find_all('li')
        else:
            result_list = []
        
        if not result_list:
            print(f"í˜ì´ì§€ {i}ì—ì„œ ìœ íš¨í•œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.")
            break
            
        for li_item in result_list:
            
            # 1. ì œëª© ë° ì›ë³¸ URL ì¶”ì¶œ
            title_tag = li_item.select_one('a.tit_txt')
            if not title_tag: continue
            
            title_raw = title_tag.get_text(strip=True)
            post_url = title_tag.get('href') # ì›ë³¸ ê²Œì‹œë¬¼ ë§í¬
            
            # 3. ê°¤ëŸ¬ë¦¬ ì´ë¦„ ì¶”ì¶œ
            meta_tag = li_item.select_one('p.link_dsc_txt.dsc_sub')
            
            # ê°¤ëŸ¬ë¦¬ ì´ë¦„
            gallery_name_tag = meta_tag.select_one('a.sub_txt') if meta_tag else None
            gallery_name = gallery_name_tag.get_text(strip=True) if gallery_name_tag else "N/A"

            # ê°¤ëŸ¬ë¦¬ ID ì¶”ì¶œ (hrefì—ì„œ id= ë’¤ì˜ ë¬¸ìì—´ì„ ì¶”ì¶œ)
            gallery_id = "N/A"
            if gallery_name_tag and gallery_name_tag.get('href'):
                gallery_list_url = gallery_name_tag.get('href')
                id_match = re.search(r'id=([^&]+)', gallery_list_url)
                gallery_id = id_match.group(1) if id_match else "N/A"
            
            # 4. PostID (ê²Œì‹œë¬¼ ê³ ìœ  ë²ˆí˜¸) ì¶”ì¶œ
            # URL ì˜ˆ: https://gall.dcinside.com/mgallery/board/view/?id=coffee&no=463912
            post_id_match = re.search(r'&no=(\d+)', post_url)
            post_id = post_id_match.group(1) if post_id_match else None
            
            # í•„ìˆ˜ ë°ì´í„°(URL, PostID)ê°€ ì—†ìœ¼ë©´ ê±´ë„ˆëœë‹ˆë‹¤.
            if not post_url or not post_id: continue

            # robots.txt disallow ê°¤ëŸ¬ë¦¬ í•„í„°ë§
            if gallery_id in DISALLOWED_IDS:
                print(f"    -> ğŸš« í•„í„°ë§: í¬ë¡¤ë§ ê¸ˆì§€ëœ ê°¤ëŸ¬ë¦¬ ID '{gallery_id}'ì˜ ê²Œì‹œë¬¼ì€ ê±´ë„ˆëœë‹ˆë‹¤.")
                continue

            # ----------------------------------------------------
            # 5. ê°œë³„ ê²Œì‹œë¬¼ ë³¸ë¬¸ ìš”ì²­ ë° ì¶”ì¶œ
            # ----------------------------------------------------
            
            # ëœë¤ ë”œë ˆì´
            time.sleep(random.uniform(3, 5))
            
            # ê²Œì‹œë¬¼ ë³¸ë¬¸ ìš”ì²­
            try:
                print(f"    -> ê²Œì‹œë¬¼ ë³¸ë¬¸ ìš”ì²­: {title_raw[:20]}... (ID: {post_id}, ê°¤ëŸ¬ë¦¬: {gallery_name})")
                article_user_agent = random.choice(USER_AGENT_LIST)
                article_headers = {'User-Agent': article_user_agent}
                # post_urlì€ ì´ë¯¸ ì ˆëŒ€ ê²½ë¡œì…ë‹ˆë‹¤.
                article_response = requests.get(post_url, headers=article_headers, timeout=10)
                article_response.raise_for_status()
            except requests.exceptions.RequestException as e:
                print(f"    -> ê²Œì‹œë¬¼ ë³¸ë¬¸ ìš”ì²­ ì‹¤íŒ¨ ({post_url}): {e}")
                continue
            
            article_soup = BeautifulSoup(article_response.content, 'lxml') 

            # ë³¸ë¬¸ ì¶”ì¶œ í´ë˜ìŠ¤: 'write_div'
            article_contents_tag = article_soup.find('div', class_='write_div')
            article_contents = ""
            if article_contents_tag:
                article_contents = article_contents_tag.get_text(strip=True)
                
            # ----------------------------------------------------
            
            # 6. ë°ì´í„° í´ë¦¬ë‹ ë° ì €ì¥
            
            # ì œëª©ê³¼ ê²Œì‹œê¸€ì—ì„œ url ì œê±°ë¥¼ ìœ„í•œ íŒ¨í„´
            pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$\-@\.&+:/?=]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
            repl = ''
            title_clean = re.sub(pattern=pattern, repl=repl, string=title_raw).strip()
            
            # ë³¸ë¬¸ í´ë¦¬ë‹
            article_contents_clean = re.sub(pattern=pattern, repl=repl, string=article_contents).strip()
            article_contents_clean = article_contents_clean.replace('- dc official App', '').strip()

            data_list.append({
                'PostID': post_id,
                'Title': title_clean,
                'Content': article_contents_clean,
                'Comments': None,
                'GalleryID': gallery_name,
                'PostURL': post_url
            })
            
    # ìµœì¢… DataFrame ë³€í™˜ ë° ì¤‘ë³µ ì œê±°
    df = pd.DataFrame(data_list)
    
    if not df.empty:
        # PostIDì™€ PostURL ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
        df = df.drop_duplicates(subset=['PostID', 'PostURL'], keep='first')
        print(f"\n--- í†µí•© ê²€ìƒ‰ í¬ë¡¤ë§ ì™„ë£Œ ë° ì¤‘ë³µ ì œê±° ---")
        print(f"ì´ ìˆ˜ì§‘ëœ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {len(data_list)}ê°œ")
        print(f"ì¤‘ë³µ ì œê±° í›„ ìµœì¢… ê²°ê³¼ ìˆ˜: {len(df)}ê°œ")

    return df