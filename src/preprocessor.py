import h2o
import pandas as pd
import os
import pickle
from datetime import datetime
from konlpy.tag import Okt
from sklearn.feature_extraction.text import TfidfVectorizer
import joblib 

# --- ê²½ë¡œ ë° í™˜ê²½ ì„¤ì • ---
# ìŠ¤í¬ë¦½íŠ¸ê°€ ìœ„ì¹˜í•œ ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# ëª¨ë¸ê³¼ ë²¡í„°ë¼ì´ì €ê°€ ìœ„ì¹˜í•  í›„ë³´ í´ë”ë“¤ (ìš°ì„ ìˆœìœ„: models í´ë” -> í˜„ì¬ í´ë”)
MODEL_DIR = os.path.join(BASE_DIR, 'models')
if not os.path.exists(MODEL_DIR):
    MODEL_DIR = BASE_DIR

# íŒŒì¼ëª… ì„¤ì • (ì‹¤ì œ ì €ì¥ëœ ëª¨ë¸ íŒŒì¼ëª…ê³¼ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤)
# ì£¼ì˜: h2o.save_model()ì€ ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë¸ IDë¥¼ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥í•˜ë¯€ë¡œ,
# ì‹¤ì œ íŒŒì¼ëª…ì„ í™•ì¸ í›„ í•„ìš”í•˜ë‹¤ë©´ ì•„ë˜ ë³€ìˆ˜ë‚˜ íŒŒì¼ëª…ì„ ìˆ˜ì •í•˜ì„¸ìš”.
MODEL_FILENAME = "GLM_Classification_Model" 
VECTORIZER_FILENAME = "tfidf_vectorizer.pkl"

# ì „ì²´ ê²½ë¡œ êµ¬ì„±
MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILENAME)
VECTORIZER_PATH = os.path.join(MODEL_DIR, VECTORIZER_FILENAME)

# í˜ì˜¤ íŒë‹¨ ì„ê³„ê°’ (í•´ë‹¹ ìˆ˜ì¹˜ ì´ìƒì´ë©´ í˜ì˜¤ë¡œ ê°„ì£¼)
HATE_THRESHOLD = 0.88 

# í˜•íƒœì†Œ ë¶„ì„ê¸° ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
okt = Okt()

def init_h2o(max_mem_size="4G"):
    """
    H2O í´ëŸ¬ìŠ¤í„°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    ì´ë¯¸ ì‹¤í–‰ ì¤‘ì´ë¼ë©´ ì—°ê²°ì„ ì‹œë„í•©ë‹ˆë‹¤.
    
    Args:
        max_mem_size (str): H2O ì¸ìŠ¤í„´ìŠ¤ ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ê¸°ë³¸ "1G")
    """
    try:
        h2o.init(max_mem_size=max_mem_size)
        print("[ì •ë³´] H2O í´ëŸ¬ìŠ¤í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"[ì˜¤ë¥˜] H2O ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

def load_resources(model_path, vectorizer_path):
    """
    ì €ì¥ëœ H2O ëª¨ë¸(Binary)ê³¼ TF-IDF ë²¡í„°ë¼ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}\n'models' í´ë”ì— íŒŒì¼ì„ ìœ„ì¹˜ì‹œí‚¤ê±°ë‚˜ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    if not os.path.exists(vectorizer_path):
        raise FileNotFoundError(f"ë²¡í„°ë¼ì´ì € íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {vectorizer_path}")

    try:
        # H2O Binary ëª¨ë¸ ë¡œë“œ
        model = h2o.load_model(model_path)
        print(f"[ì •ë³´] ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {os.path.basename(model_path)}")
        
        # Vectorizer ë¡œë“œ
        with open(vectorizer_path, 'rb') as f:
            try:
                vectorizer = pickle.load(f)
            except Exception:
                # pickle ë¡œë”© ì‹¤íŒ¨ ì‹œ joblib ì‹œë„ (í˜¸í™˜ì„± í™•ë³´)
                try:
                    vectorizer = joblib.load(f)
                except Exception:
                    raise IOError("Vectorizer ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤ (pickle/joblib ëª¨ë‘ ì‹¤íŒ¨).")

        print(f"[ì •ë³´] ë²¡í„°ë¼ì´ì € ë¡œë“œ ì™„ë£Œ: {os.path.basename(vectorizer_path)}")
        
        return model, vectorizer
    except Exception as e:
        print(f"[ì˜¤ë¥˜] ë¦¬ì†ŒìŠ¤ ë¡œë“œ ì¤‘ ë¬¸ì œ ë°œìƒ: {e}")
        raise

def tokenize(text):
    """
    KoNLPy Oktë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•©ë‹ˆë‹¤.
    ì¡°ì‚¬, ì–´ë¯¸, êµ¬ë‘ì ì„ ì œì™¸í•˜ê³  ì–´ê°„(Stem)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    if pd.isna(text) or not isinstance(text, str):
        return ""
    
    EXCLUDE_POS = ['Josa', 'Eomi', 'Punctuation']
    try:
        # stem=True ì˜µì…˜ìœ¼ë¡œ ì–´ê°„ ì¶”ì¶œ
        tokens = [word for word, pos in okt.pos(text, stem=True) if pos not in EXCLUDE_POS]
        return " ".join(tokens)
    except Exception as e:
        # ì—ëŸ¬ ë°œìƒ ì‹œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜ ë° ë¡œê·¸ ì¶œë ¥
        print(f"[ê²½ê³ ] í† í°í™” ì˜¤ë¥˜ (í…ìŠ¤íŠ¸: '{text[:20]}...'): {e}")
        return ""

def batch_predict(texts, model, vectorizer):
    """
    ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì…ë ¥ë°›ì•„ ì¼ê´„(Batch)ë¡œ í˜ì˜¤ í™•ë¥ ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    ì†ë„ í–¥ìƒì„ ìœ„í•´ H2Oì˜ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í™œìš©í•©ë‹ˆë‹¤.
    """
    if not texts:
        return []
    
    # 1. ì¼ê´„ í† í°í™” (ê°€ì¥ ì‹œê°„ì´ ë§ì´ ì†Œìš”ë˜ëŠ” ì‘ì—…)
    print(f"[ì§„í–‰] {len(texts)}ê°œ í…ìŠ¤íŠ¸ í•­ëª© í† í°í™” ì¤‘...")
    tokenized_texts = [tokenize(text) for text in texts]
    
    # 2. ë²¡í„°í™” (Sparse Matrix ìƒì„±)
    # ë°ì´í„°ê°€ ë§¤ìš° í´ ê²½ìš° ë©”ëª¨ë¦¬ ì´ìŠˆê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë‚˜, ì¼ë°˜ì ì¸ CSV ê·œëª¨ì—ì„œëŠ” ë¬¸ì œì—†ìŒ
    try:
        X_vec = vectorizer.transform(tokenized_texts)
    except Exception as e:
        print(f"[ì˜¤ë¥˜] ë²¡í„°í™” ë³€í™˜ ì‹¤íŒ¨: {e}")
        return [0.0] * len(texts)
        
    # 3. H2OFrame ë³€í™˜ ì¤€ë¹„
    # feature_namesëŠ” í•™ìŠµ ë•Œì™€ ë™ì¼í•´ì•¼ í•¨
    feature_names = [f'feature_{i}' for i in range(X_vec.shape[1])]
    
    # Sparse Matrix -> Dense DataFrame -> H2OFrame
    # (ì°¸ê³ : ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ ì‹œ ì´ ë¶€ë¶„ì—ì„œ ë©”ëª¨ë¦¬ ìµœì í™”ê°€ í•„ìš”í•  ìˆ˜ ìˆìŒ)
    X_df = pd.DataFrame(X_vec.toarray(), columns=feature_names)
    hf = h2o.H2OFrame(X_df)
    
    # 4. H2O ì˜ˆì¸¡ ì‹¤í–‰
    print("[ì§„í–‰] H2O ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
    predictions = model.predict(hf)
    result = predictions.as_data_frame(use_multi_thread=True)
    
    # 5. ê²°ê³¼ ì¶”ì¶œ (í˜ì˜¤ì¼ í™•ë¥ )
    # ëª¨ë¸ì— ë”°ë¼ í™•ë¥  ì»¬ëŸ¼ëª…ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìˆœì°¨ì ìœ¼ë¡œ í™•ì¸
    if 'hate' in result.columns:
        return result['hate'].tolist()
    elif 'p1' in result.columns:
        return result['p1'].tolist()
    
    # ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° ë””ë²„ê¹… ì •ë³´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    print(f"[ì˜¤ë¥˜] ì˜ˆì¸¡ ê²°ê³¼ì— í™•ë¥  ì»¬ëŸ¼('hate' ë˜ëŠ” 'p1')ì´ ì—†ìŠµë‹ˆë‹¤.")
    print(f"      ë°œê²¬ëœ ì»¬ëŸ¼ ëª©ë¡: {result.columns.tolist()}")
    return [0.0] * len(texts)

def filter_hate_speech(df, model_path=MODEL_PATH, vectorizer_path=VECTORIZER_PATH):
    """
    ë°ì´í„°í”„ë ˆì„ì˜ Title, Content, Commentsë¥¼ ê²€ì‚¬í•˜ì—¬ í˜ì˜¤ í‘œí˜„ì„ í•„í„°ë§í•©ë‹ˆë‹¤.
    ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ëª¨ì•„ì„œ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ë¯€ë¡œ ì†ë„ê°€ ë¹ ë¦…ë‹ˆë‹¤.
    """
    # ğŸŒŸğŸŒŸğŸŒŸ ìˆ˜ì • ì§€ì : h2o.init ì˜µì…˜ì„ ì—¬ê¸°ì— ì§ì ‘ ì „ë‹¬í•©ë‹ˆë‹¤. ğŸŒŸğŸŒŸğŸŒŸ
    # ipì™€ start_h2o ì˜µì…˜ì„ init_h2o í•¨ìˆ˜ì— ì „ë‹¬í•˜ì—¬ H2O í´ëŸ¬ìŠ¤í„°ì— ì—°ê²°ì„ ì‹œë„í•©ë‹ˆë‹¤.
    init_h2o()
    
    model, vectorizer = load_resources(model_path, vectorizer_path)
    
    # 1. ëª¨ë“  í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ë° ì¸ë±ì‹±
    all_texts_to_predict = []
    text_map = [] # (í–‰ ì¸ë±ìŠ¤, í•­ëª© íƒ€ì…, ëŒ“ê¸€ ì¸ë±ìŠ¤)

    for index, row in df.iterrows():
        # ì œëª© (Title)
        all_texts_to_predict.append(row.get('Title', ''))
        text_map.append((index, 'Title', None))

        # ë³¸ë¬¸ (Content)
        all_texts_to_predict.append(row.get('Content', ''))
        text_map.append((index, 'Content', None))

        # ëŒ“ê¸€ (Comments) - ' ||| 'ë¡œ êµ¬ë¶„ë¨
        comments_str = row.get('Comments', '')
        if pd.notna(comments_str) and comments_str:
            comment_list = comments_str.split(' ||| ')
            for c_idx, comment in enumerate(comment_list):
                comment = comment.strip()
                # ë¹ˆ ëŒ“ê¸€ì´ë‚˜ http ë§í¬ëŠ” ì˜ˆì¸¡ì—ì„œ ì œì™¸ (ì†ë„ ìµœì í™”)
                if comment and not comment.lower().startswith('http'):
                    all_texts_to_predict.append(comment)
                    text_map.append((index, 'Comment', c_idx))
    
    if not all_texts_to_predict:
        print("[ì •ë³´] ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return df
        
    # 2. ë°°ì¹˜ ì˜ˆì¸¡ ì‹¤í–‰
    print(f"[ì‹œì‘] ì´ {len(all_texts_to_predict)}ê°œ í•­ëª©ì— ëŒ€í•œ ë°°ì¹˜ ë¶„ì„ ì‹œì‘...")
    # ìˆ˜ì •ëœ batch_predict í˜¸ì¶œ ('hate' ë˜ëŠ” 'p1' ì»¬ëŸ¼ ê°’ ìë™ ê°ì§€)
    hate_probs = batch_predict(all_texts_to_predict, model, vectorizer)
    print("[ì™„ë£Œ] ë°°ì¹˜ ì˜ˆì¸¡ ì™„ë£Œ.")

    # 3. ì˜ˆì¸¡ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”
    # ì¸ë±ìŠ¤ë³„ë¡œ ê²°ê³¼ë¥¼ ê·¸ë£¹í™”í•˜ì—¬ ì‰½ê²Œ ì¡°íšŒí•  ìˆ˜ ìˆê²Œ ë§Œë“­ë‹ˆë‹¤.
    prediction_results = list(zip(text_map, hate_probs))
    
    # ê¸°ë³¸ê°’ ì„¤ì •
    row_data = {index: {'Title': 0.0, 'Content': 0.0, 'Comments': {}} for index in df.index}
    
    for (r_idx, item_type, c_idx), p_score in prediction_results:
        if item_type == 'Title':
            row_data[r_idx]['Title'] = p_score
        elif item_type == 'Content':
            row_data[r_idx]['Content'] = p_score
        elif item_type == 'Comment':
            row_data[r_idx]['Comments'][c_idx] = p_score

    # 4. í•„í„°ë§ ë¡œì§ ì ìš©
    filtered_rows = []
    dropped_rows = []
    
    for index, row in df.iterrows():
        original_row = row.copy()
        current_data = row_data[index]
        
        # 4.1. ì œëª© ë° ë³¸ë¬¸ í˜ì˜¤ ì²´í¬ (ë°œê²¬ ì‹œ í–‰ ì „ì²´ ì‚­ì œ)
        if current_data['Title'] >= HATE_THRESHOLD:
            dropped_rows.append({'reason': 'Title Hate', 'p_hate': current_data['Title'], 'data': original_row.to_dict()})
            continue
            
        if current_data['Content'] >= HATE_THRESHOLD:
            dropped_rows.append({'reason': 'Content Hate', 'p_hate': current_data['Content'], 'data': original_row.to_dict()})
            continue

        # 4.2. ëŒ“ê¸€ í•„í„°ë§ (í˜ì˜¤ ëŒ“ê¸€ë§Œ ì œê±°í•˜ê³  í–‰ì€ ìœ ì§€)
        comments_str = row.get('Comments', '')
        
        if pd.notna(comments_str) and comments_str:
            comment_list = comments_str.split(' ||| ')
            clean_comments_list = []
            
            # ì˜ˆì¸¡ ê²°ê³¼ ë§µ
            comment_predictions = current_data['Comments']

            for c_idx, comment in enumerate(comment_list):
                comment = comment.strip()
                if not comment: continue

                # ë§í¬ í•„í„°ë§ (ì˜ˆì¸¡ ê±´ë„ˆë›´ í•­ëª©)
                if comment.lower().startswith('http'):
                    continue

                # í˜ì˜¤ ì ìˆ˜ í™•ì¸ (ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ì§€ ì•Šì€ í•­ëª©ì€ 0.0)
                p_score = comment_predictions.get(c_idx, 0.0)
                
                if p_score >= HATE_THRESHOLD:
                    # í˜ì˜¤ ëŒ“ê¸€ ê¸°ë¡
                    dropped_rows.append({
                        'reason': 'Comment Hate', 
                        'p_hate': p_score, 
                        'data': {'PostID': row.get('PostID'), 'Comment': comment}
                    })
                else:
                    # ì •ìƒ ëŒ“ê¸€ ìœ ì§€
                    clean_comments_list.append(comment)
            
            # ì •ì œëœ ëŒ“ê¸€ë¡œ ì—…ë°ì´íŠ¸
            row['Comments'] = " ||| ".join(clean_comments_list)
        
        filtered_rows.append(row)
        
    # 5. ê²°ê³¼ ì €ì¥ ë° ë¡œê·¸ ì¶œë ¥
    filtered_df = pd.DataFrame(filtered_rows)
    
    # ì œê±°ëœ í•­ëª© ë¡œê·¸ ì €ì¥
    if dropped_rows:
        log_dir = os.path.join(BASE_DIR, 'logs')
        if not os.path.exists(log_dir):
            os.makedirs(log_dir)
            
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = os.path.join(log_dir, f'dropped_hate_speech_{timestamp}.csv')
        
        log_df = pd.DataFrame([
            {'Reason': item['reason'], 'PHate': item['p_hate'], **item['data']} 
            for item in dropped_rows
        ])
        
        # [ìˆ˜ì •] ì˜¤ì§ ë¡œê·¸ íŒŒì¼(ì‚¬ëŒ í™•ì¸ìš©)ì— ëŒ€í•´ì„œë§Œ ì¤„ë°”ê¿ˆ ì œê±° ì ìš©
        # Title, Content, Comments, Comment ë“± í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ë©´ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜
        if not log_df.empty:
            for col in ['Title', 'Content', 'Comments', 'Comment']: 
                if col in log_df.columns:
                    log_df[col] = log_df[col].apply(lambda x: x.replace('\n', ' ') if isinstance(x, str) else x)

        log_df.to_csv(log_file, index=False, encoding='utf-8-sig')
        print(f"[ê²°ê³¼] {len(dropped_rows)}ê°œì˜ í˜ì˜¤ ì½˜í…ì¸ ê°€ í•„í„°ë§ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"      ìƒì„¸ ë¡œê·¸: {log_file}")
        
    # H2O ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    if h2o.cluster().get_status() == "running":
        h2o.cluster().shutdown()
        
    return filtered_df

# --- ì‹¤í–‰ ì˜ˆì‹œ (Github ì—…ë¡œë“œ ì‹œ ì‚¬ìš©ì ê°€ì´ë“œìš©) ---
if __name__ == "__main__":
    # ë°ì´í„° í´ë” ê²½ë¡œ ì„¤ì • (data í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±í•˜ê±°ë‚˜ ê²½ë¡œ ìˆ˜ì • í•„ìš”)
    DATA_DIR = os.path.join(BASE_DIR, 'data')
    INPUT_FILE = "test_ARCA_breaking_ë§¨ìœ .csv" # í…ŒìŠ¤íŠ¸ìš© íŒŒì¼ëª…
    INPUT_PATH = os.path.join(DATA_DIR, INPUT_FILE)
    
    print(f"--- í˜ì˜¤ í‘œí˜„ í•„í„°ë§ ì‹œì‘ ---")
    print(f"ì…ë ¥ íŒŒì¼ ê²½ë¡œ: {INPUT_PATH}")
    
    if os.path.exists(INPUT_PATH):
        try:
            # ë°ì´í„° ë¡œë“œ
            df = pd.read_csv(INPUT_PATH)
            
            # í•„í„°ë§ ìˆ˜í–‰
            filtered_df = filter_hate_speech(df)
            
            # ê²°ê³¼ ì €ì¥
            OUTPUT_FILE = f"filtered_{INPUT_FILE}"
            OUTPUT_PATH = os.path.join(DATA_DIR, OUTPUT_FILE)
            
            filtered_df.to_csv(OUTPUT_PATH, index=False, encoding='utf-8-sig')
            print(f"[ì™„ë£Œ] í•„í„°ë§ëœ ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {OUTPUT_PATH}")
            
        except Exception as e:
            print(f"[ì˜¤ë¥˜] ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
    else:
        print(f"[ê²½ê³ ] ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. '{DATA_DIR}' í´ë”ì— '{INPUT_FILE}'ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")