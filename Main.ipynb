{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b058577-9fe6-4819-b2a9-a84405aaddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time    # 랜덤 딜레이시\n",
    "import random  # 랜덤 딜레이시\n",
    "import re # 정규 표현식\n",
    "import pandas as pd # 데이터프레임 사용\n",
    "\n",
    "# ----------------------\n",
    "# 1. 상수 정의 (PC 버전)\n",
    "# ----------------------\n",
    "BASE_URL = \"https://gall.dcinside.com/mgallery/board/lists\"\n",
    "ARTICLE_BASE_URL = \"https://gall.dcinside.com\"\n",
    "\n",
    "# User-Agent 목록 정의(랜덤선택)\n",
    "USER_AGENT_LIST = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 14_2_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "]\n",
    "\n",
    "\n",
    "def get_regular_post_data(gallery_id: str, search_keyword: str = \"\", start_page: int = 1, end_page: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    PC 갤러리 페이지에서 게시물의 제목과 내용을 추출하여 DataFrame으로 반환합니다.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(columns=['Title', 'Content', 'GalleryID', 'URL'])\n",
    "    \n",
    "    for i in range(start_page, end_page + 1):\n",
    "        \n",
    "        # ----------------------\n",
    "        # 1단계: 목록 페이지 요청 및 파싱\n",
    "        # ----------------------\n",
    "        \n",
    "        params = {'id': gallery_id, 'page': i}\n",
    "\n",
    "        # 검색 주소 조립 시 필요한 파라미터 정의\n",
    "        # ex) https://gall.dcinside.com/mgallery/board/lists/?id=warship&s_type=search_subject_memo&s_keyword=알래스카\n",
    "        if search_keyword:\n",
    "            # PC 검색 파라미터 사용\n",
    "            params['search_pos'] = ''\n",
    "            params['s_type'] = 'search_subject_memo'\n",
    "            params['s_keyword'] = search_keyword\n",
    "\n",
    "        # User-Agent 설정\n",
    "        user_agent = random.choice(USER_AGENT_LIST)\n",
    "        headers = {'User-Agent': user_agent}\n",
    "\n",
    "        # try-except\n",
    "        try:\n",
    "            print(f\"--- 갤러리 목록 페이지 {i} 요청 중 ---\")\n",
    "            response = requests.get(BASE_URL, params=params, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"목록 페이지 {i} 요청 실패: {e}. 다음 페이지로 이동합니다.\")\n",
    "            time.sleep(random.uniform(2, 4))\n",
    "            continue\n",
    "\n",
    "        # lxml 파서 사용(HTML 대신)\n",
    "        soup = BeautifulSoup(response.content, 'lxml')\n",
    "        \n",
    "        # 글 목록 구조: <tbody> 내의 <tr>\n",
    "        article_list = soup.find('tbody').find_all('tr', {'data-type': ['icon_pic', 'icon_txt']})\n",
    "        \n",
    "        # 기본 공지, 광고글 필터링\n",
    "        filtered_articles = []\n",
    "        for tr_item in article_list:\n",
    "            writer_tag = tr_item.find('td', class_='gall_writer')\n",
    "            is_operator_post = writer_tag and writer_tag.get('user_name') == '운영자'\n",
    "            is_notice = tr_item.get('data-type') == 'icon_notice'\n",
    "            \n",
    "            if not is_operator_post and not is_notice:\n",
    "                filtered_articles.append(tr_item)\n",
    "                \n",
    "        if not filtered_articles:\n",
    "             print(f\"페이지 {i}에서 유효한 일반 게시물이 없습니다. 크롤링 종료.\")\n",
    "             break \n",
    "\n",
    "\n",
    "        # ----------------------\n",
    "        # 2단계: 개별 게시물 접근 및 내용 추출 \n",
    "        # ----------------------\n",
    "        for tr_item in filtered_articles:\n",
    "            \n",
    "            title_tag = tr_item.find('a', href=True)\n",
    "            if not title_tag: continue\n",
    "\n",
    "            title_raw = title_tag.text.strip()\n",
    "            relative_url = title_tag['href']\n",
    "            \n",
    "            # href 절대 경로/상대 경로 모두 대응 (없어도 솔직히 문제 없을듯?)\n",
    "            if relative_url.startswith('http'):\n",
    "                full_url = relative_url\n",
    "            else:\n",
    "                full_url = ARTICLE_BASE_URL + relative_url\n",
    "\n",
    "            # 랜덤 딜레이\n",
    "            time.sleep(random.uniform(3, 5))\n",
    "            \n",
    "            # 게시물 본문 요청\n",
    "            try:\n",
    "                print(f\"   -> 게시물 요청: {title_raw[:20]}...\")\n",
    "                article_user_agent = random.choice(USER_AGENT_LIST)\n",
    "                article_headers = {'User-Agent': article_user_agent}\n",
    "                article_response = requests.get(full_url, headers=article_headers, timeout=10)\n",
    "                article_response.raise_for_status()\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"   -> 게시물 요청 실패 ({full_url}): {e}\")\n",
    "                continue\n",
    "            \n",
    "            article_soup = BeautifulSoup(article_response.content, 'lxml') # lxml 사용\n",
    "\n",
    "            # 본문 추출 클래스: 'write_div'\n",
    "            article_contents_tag = article_soup.find('div', class_='write_div')\n",
    "            article_contents = \"\"\n",
    "            if article_contents_tag:\n",
    "                # 텍스트만 추출하고 불필요한 공백 제거\n",
    "                article_contents = BeautifulSoup(str(article_contents_tag), \"lxml\").text.strip()\n",
    "            \n",
    "            # ----------------------\n",
    "            # 3단계: 데이터 클리닝 및 저장\n",
    "            # ----------------------\n",
    "            \n",
    "            # 제목과 게시글에서 url 제거\n",
    "            pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$\\-@\\.&+:/?=]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "            repl = ''\n",
    "            title_clean = re.sub(pattern=pattern, repl=repl, string=title_raw).strip()\n",
    "            article_contents_clean = re.sub(pattern=pattern, repl=repl, string=article_contents).strip()\n",
    "            \n",
    "            # '- dc official App' 제거\n",
    "            article_contents_clean = article_contents_clean.replace('- dc official App', '').strip()\n",
    "            \n",
    "            \n",
    "            if article_contents_clean:\n",
    "                \n",
    "                new_row = pd.DataFrame([{\n",
    "                    'Title': title_clean,\n",
    "                    'Content': article_contents_clean,\n",
    "                    'GalleryID': gallery_id,\n",
    "                    'URL': full_url\n",
    "                }])\n",
    "                \n",
    "                df = pd.concat([df, new_row], ignore_index=True)\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "73a09adb-559d-454c-acb9-deb966c56f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 갤러리 목록 페이지 1 요청 중 ---\n",
      "   -> 게시물 요청: 알래스카 피해 복구반 어떻게 써야함?...\n",
      "   -> 게시물 요청: 리코 잘타는 법 좀 ㅠㅠㅠ...\n",
      "   -> 게시물 요청: 블프상자 25개로 알래스카 사이판먹었...\n",
      "   -> 게시물 요청: 알래스카 메추리 사는이유...\n",
      "   -> 게시물 요청: 결국 알래스카 삿다...\n",
      "   -> 게시물 요청: 알래스카 지르기전 마지막 질문, 진짜...\n",
      "   -> 게시물 요청: PTS 작전 밸런스 조정 하는듯?...\n",
      "   -> 게시물 요청: 25% 골드쿠폰 소진용 골쉽 추천점...\n",
      "   -> 게시물 요청: 블프 아타고, 마인츠는 관짝 들어갔다...\n",
      "   -> 게시물 요청: 결국 키어사지도 샀다...\n",
      "   -> 게시물 요청: 원래 알래스카 살랬는데...\n",
      "   -> 게시물 요청: 근데 알래스카 있으면 좋음...\n",
      "   -> 게시물 요청: 소신발언)알래스카 이제 필구급은 아님...\n",
      "   -> 게시물 요청: 뉴비가 대순타면 불타뒤진다고 알래스카...\n",
      "   -> 게시물 요청: 중고 뉴비인데  알래스카 진짜 좆냐?...\n",
      "   -> 게시물 요청: 알래스카가 진짜 명품이네...\n",
      "   -> 게시물 요청: 쉽붕이 순양 입문으로 알래스카 샀는데...\n",
      "   -> 게시물 요청: 조지아집탄으로도 포격연습이 잘 안되더...\n",
      "   -> 게시물 요청: 뉴비 9티어 배 고민이요ㅜ...\n",
      "   -> 게시물 요청: 근데 알래스카 현금가격 올라도...\n",
      "--- 갤러리 목록 페이지 2 요청 중 ---\n",
      "   -> 게시물 요청: 만토이펠이 알래스카 상위호환 같은데...\n",
      "   -> 게시물 요청: 알래스카에 환상 가진 뉴비들이 많네...\n",
      "   -> 게시물 요청: 아니씹 알래스카 가격 올렸네...\n",
      "   -> 게시물 요청: 알래스카 가격 또 올랐네 아 ㅋㅋㅋㅋ...\n",
      "   -> 게시물 요청: 뉴비 알래스카 철갑탄 질문좀...\n",
      "   -> 게시물 요청: 혹시 1주전 워쉽 블프 가격목록 아시...\n",
      "   -> 게시물 요청: 클전 밴픽을 게임사가 자체적으로 박는...\n",
      "   -> 게시물 요청: 브위스52 사기배 맞고 고인물들 자원...\n",
      "   -> 게시물 요청: 골쉽이나 특군끼리는 함장 돌려쓸 수 ...\n",
      "   -> 게시물 요청: 알래스카 뽑았다...\n",
      "   -> 게시물 요청: 로드섬 사기배 아니지?...\n",
      "   -> 게시물 요청: 와 대놓고 에임핵 뻔하게 플레이 하는...\n",
      "   -> 게시물 요청: 9티어에 수리반 달린 플레쳐 하나 안...\n",
      "   -> 게시물 요청: 알래스카가 대순 GOAT인 이유...\n",
      "   -> 게시물 요청: 어제 지크 알래스카 갈드컵 있었구나....\n",
      "   -> 게시물 요청: 알래스카 2200-2300은 전문가 ...\n",
      "   -> 게시물 요청: 로드섬이랑 알래스카 중에 고르라하면 ...\n",
      "   -> 게시물 요청: 알래스카 함장스킬 어떻게들 찍음?...\n",
      "   -> 게시물 요청: 해명...\n",
      "   -> 게시물 요청: 순양함이 능동적으로 겜 풀어나갈거면 ...\n",
      "최종 수집된 게시물 수: 40개\n",
      "\n",
      "데이터가 dcinside_data.csv 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: 함수 실행 및 결과 확인\n",
    "\n",
    "# 1. 갤러리 ID와 검색어 설정\n",
    "gallery_id_target = 'warship'\n",
    "search_keyword_target = '알래스카'\n",
    "start_page_num = 1\n",
    "end_page_num = 2 # 1페이지부터 3페이지까지 수집\n",
    "\n",
    "# 2. 함수 호출\n",
    "results_df = get_regular_post_data(\n",
    "    gallery_id=gallery_id_target, \n",
    "    search_keyword=search_keyword_target, \n",
    "    start_page=start_page_num, \n",
    "    end_page=end_page_num\n",
    ")\n",
    "\n",
    "# 3. 결과 출력 및 확인\n",
    "print(f\"최종 수집된 게시물 수: {len(results_df)}개\")\n",
    "\n",
    "# 주피터 노트북은 head()를 호출하면 DataFrame을 테이블 형태로 예쁘게 출력합니다.\n",
    "results_df.head()\n",
    "\n",
    "# CSV 저장 (선택 사항)\n",
    "if not results_df.empty:\n",
    "    results_df.to_csv(\"dcinside_data.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"\\n데이터가 dcinside_data.csv 파일로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98d7ebf-0fa4-483b-a1f8-128c98c5ab9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
