import os
import json
import time
import re
import pandas as pd
import streamlit as st
import google.generativeai as genai
import concurrent.futures
from dotenv import load_dotenv

# --------------------------------------------------------------------------
# 1. ì´ˆê¸° ì„¤ì • ë° í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# --------------------------------------------------------------------------
load_dotenv()

st.set_page_config(
    page_title="Community Insight Bot",
    page_icon="ğŸ•µï¸â€â™‚ï¸",
    layout="wide"
)

# ìƒˆë¡œìš´ ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from src.crawler_wrapper import search_community
    from src.preprocessor import filter_hate_speech
except ImportError as e:
    st.error(f"í•„ìˆ˜ ëª¨ë“ˆì„ ì„í¬íŠ¸í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    st.stop()

# --------------------------------------------------------------------------
# 2. ì‚¬ì´ë“œë°” ì„¤ì • (ì»¤ë®¤ë‹ˆí‹° ì„ íƒ ì œê±°, API í‚¤ í™•ì¸ ìœ ì§€)
# --------------------------------------------------------------------------
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    
    # API í‚¤ ë° ëª¨ë¸ ì„¤ì • í™•ì¸ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    if not os.getenv("API_KEY"):
        st.error("ğŸš¨ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    if not os.getenv("MODEL"):
        st.warning("âš ï¸ ëª¨ë¸ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
    st.info("AIê°€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ìë™ìœ¼ë¡œ ì»¤ë®¤ë‹ˆí‹°(DC/Arca)ë¥¼ ì„ ì •í•˜ê³  ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    st.markdown("---")
    st.caption("Powered by Google Gemini")

# --------------------------------------------------------------------------
# 3. Gemini ëª¨ë¸ ë¡œë“œ 
# --------------------------------------------------------------------------
@st.cache_resource
def get_gemini_model():
    """
    Gemini ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤. 
    st.cache_resourceë¥¼ ì‚¬ìš©í•˜ì—¬ ì„¸ì…˜ ê°„ ëª¨ë¸ ê°ì²´ë¥¼ ê³µìœ í•©ë‹ˆë‹¤.
    """
    YOUR_API_KEY = os.getenv("API_KEY")
    if not YOUR_API_KEY:
        st.error("ğŸš¨ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()

    YOUR_MODEL = os.getenv("MODEL")
    if not YOUR_MODEL:
        st.error("ëª¨ë¸ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. '.env' íŒŒì¼ì— 'MODEL'ì„ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        st.stop()
        
    genai.configure(api_key=YOUR_API_KEY)
    
    # ì•ˆì „ ì„¤ì •: ëª¨ë“  ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ ì°¨ë‹¨ ì—†ìŒ(BLOCK_NONE)ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì˜¤íƒì§€ ë°©ì§€
    safety_settings = [
        {
            "category": "HARM_CATEGORY_HARASSMENT",
            "threshold": "BLOCK_NONE"
        },
        {
            "category": "HARM_CATEGORY_HATE_SPEECH",
            "threshold": "BLOCK_NONE"
        },
        {
            "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
            "threshold": "BLOCK_NONE"
        },
        {
            "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
            "threshold": "BLOCK_NONE"
        },
    ]
    
    return genai.GenerativeModel(YOUR_MODEL, safety_settings=safety_settings)

# --------------------------------------------------------------------------
# 4. í•µì‹¬ ë¡œì§ í•¨ìˆ˜ 
# --------------------------------------------------------------------------

def get_search_plan(user_input):
    """
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê²€ìƒ‰ ê³„íšì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤.
    """
    model = get_gemini_model()
    
    system_instruction = """
    ë„ˆëŠ” 'ì»¤ë®¤ë‹ˆí‹° í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ìœ„í•œ ê²€ìƒ‰ ì„¤ê³„ì'ì•¼. 
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•´ì„œ ì–´ë–¤ ì»¤ë®¤ë‹ˆí‹°(DCInside, ArcaLive ë‘˜ ì¤‘ í•˜ë‚˜)ë¥¼ ì–´ë–¤ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í• ì§€ êµ¬ì²´ì ì¸ ê³„íšì„ ì„¸ì›Œì¤˜.
    
    [í•„ìˆ˜ ê·œì¹™]
    1. ì‚¬ìš©ìê°€ "ì—¬ë¡ ", "ë°˜ì‘", "í‰ê°€" , "ì˜ê²¬" , "ê·¼í™©" ë“± í‚¤ì›Œë“œ ê²€ìƒ‰ì´ ê°€ëŠ¥í•œ ê²ƒì„ ë¬¼ìœ¼ë©´ mode="search"ë¡œ ì„¤ì •í•´.
    2. **ê²€ìƒ‰ì–´(keyword)ëŠ” ê³µì‹ ëª…ì¹­ë³´ë‹¤ ì‹¤ì œë¡œ ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ë§ì´ ì“°ì´ëŠ” 'ì€ì–´'ë‚˜ 'ì¤„ì„ë§'ì„ ìš°ì„ ì ìœ¼ë¡œ ì„ íƒí•´.** (ì˜ˆ: ë¸”ë£¨ ì•„ì¹´ì´ë¸Œ -> ë¸”ì•„, ë¦¬ê·¸ì˜¤ë¸Œë ˆì „ë“œ -> ë¡¤, ë§¨ì²´ìŠ¤í„° ìœ ë‚˜ì´í‹°ë“œ -> ë§¨ìœ )
    3. **[íƒ€ê²Ÿ ì„ ì •]** íŠ¹ì • ì‚¬ì´íŠ¸ ì–¸ê¸‰ì´ ì—†ìœ¼ë©´, í•´ë‹¹ ì£¼ì œê°€ í™œë°œí•œ ê³³ì„ ìë™ìœ¼ë¡œ íŒë‹¨í•˜ë˜ **ì˜ ëª¨ë¥´ê² ê±°ë‚˜ ëŒ€ì¤‘ì ì¸ ê²Œì„/ì´ìŠˆë¼ë©´ ["dc", "arca"] ë‘ ê³³ì— ëŒ€í•´ taskë¥¼ ì´ 2ê°œ ìƒì„±í•´ì•¼í•´.**
    4. ì¼ë°˜ì ì¸ ê²½ìš°ì—ëŠ” í†µí•©ê²€ìƒ‰ì„ ìš°ì„ ì‹œ í•˜ë˜, ëª…í™•í•œ íƒ€ê²Ÿ ê°¤ëŸ¬ë¦¬ê°€ ì¡´ì¬í•  ê²½ìš°(DCInsideëŠ” 'gallery_id'ë¥¼ í™•ì¸í•´ì„œ task['options']ì— í¬í•¨í•´ì•¼ í•´.
    5. ì‘ë‹µì€ ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ì— ë§ì¶°ì„œ ë°˜í™˜í•´ì¤˜.

    [mode íŒë‹¨ ê¸°ì¤€]
    1. "search": íŠ¹ì • ê²Œì„, ì¸ë¬¼, ì‚¬ê±´ ë“± í‚¤ì›Œë“œ ê²€ìƒ‰ì´ ê°€ëŠ¥í•œ ì£¼ì œì— ëŒ€í•´ ë¬»ëŠ” ê²½ìš°.
    2. "clarify": í‚¤ì›Œë“œê°€ ë„ˆë¬´ ëª¨í˜¸í•´ì„œ(ì˜ˆ: 'í—¤ë¥´íƒ€'ê°€ ì‘ê°€ í—¤ë¥´íƒ€ ë®ëŸ¬ì¸ì§€, ì¶•êµ¬íŒ€ í—¤ë¥´íƒ€ BSCì¸ì§€, ë¶•ê´´ ìŠ¤íƒ€ë ˆì¼ ê²Œì„ì˜ ë“±ì¥ì¸ë¬¼ í—¤ë¥´íƒ€ì¸ì§€ ë¶ˆë¶„ëª…í•¨) ê²€ìƒ‰ ëŒ€ìƒì„ í™•ì •í•  ìˆ˜ ì—†ëŠ” ê²½ìš°.
    3. "chat": ë‹¨ìˆœ ì¸ì‚¬, ì¡ë‹´, í˜¹ì€ ë¶„ì„ê³¼ ê´€ë ¨ ì—†ëŠ” ëŒ€í™”.

    [JSON ì¶œë ¥ í˜•ì‹]
    {
        "mode": "search" | "clarify" | "chat", 
        "reply_message": "clarify" | "chat" ëª¨ë“œì¼ ë•Œ ì‚¬ìš©ìì—ê²Œ í•  ë§,
        "tasks": [
            {
                "target_source": "dc" | "arca",
                "keyword": "ê²€ìƒ‰ì–´, ê²€ìƒ‰ê²°ê³¼ê°€ ì¢‹ì„ ê²ƒì´ë¼ê³  ìƒê°ë˜ë©´ ì€ì–´, ì¤„ì„ë§ ì ê·¹ì ìœ¼ë¡œ ì‚¬ìš©. íŠ¹ì • ì£¼ì œì˜ ê°¤ëŸ¬ë¦¬ ê²€ìƒ‰ ì‹œ ì£¼ì œ ê´€ë ¨ ë‹¨ì–´ëŠ” ì œê±°. (ì˜ˆ: ì›ì‹  ê°¤ëŸ¬ë¦¬ ê²€ìƒ‰ ì‹œ, 'ì›ì‹  í•„ìˆ˜ìº' -> 'í•„ìˆ˜ìº')",
                "options": {
                    # dc, arca ê³µí†µ íŒŒë¼ë¯¸í„°
                    "end_page": "ì¢…ë£Œ í˜ì´ì§€ (ì»¤ë®¤ë‹ˆí‹° í•˜ë‚˜ë§Œ íƒìƒ‰í•  ë•ŒëŠ” '2', ë‘ ê³³ ëª¨ë‘(len(tasks) == 2)ì¼ ë•ŒëŠ” '1')",

                    # "target_source" == "dc" ì¼ ë•Œ ì…ë ¥í•  ë‚´ìš©
                    "gallery_id": "í‚¤ì›Œë“œë¥¼ ê²€ìƒ‰í•  ê°¤ëŸ¬ë¦¬ì˜ ê°¤ëŸ¬ë¦¬ ID (ì˜ˆ: 'maplestory_new', 'leagueoflegends6', 'chzzk'). ëª¨ë¥´ê±°ë‚˜ í†µí•©ê²€ìƒ‰ì´ ì í•©í•  ê²½ìš° null ",
                    "gallery_type": "gallery_idê°’ì— í•´ë‹¹í•˜ëŠ” ê°¤ëŸ¬ë¦¬ì˜ ì¢…ë¥˜ë¡œ ë‹¤ìŒ ë‘˜ ì¤‘ í•˜ë‚˜ ('major' | 'minor'). í†µí•©ê²€ìƒ‰ì´ ì í•©í•  ê²½ìš° null", 
                    "sort_type": "latest"

                    # "target_source" == "arca" ì¼ ë•Œ ì…ë ¥í•  ë‚´ìš©
                    "channel_id": "breaking" (í•­ìƒ í†µí•©ê²€ìƒ‰ ì‚¬ìš©),
                }
            }
        ]
    }
    """
    
    try:
        response = model.generate_content(
            f"{system_instruction}\n\nUser Input: {user_input}",
            generation_config={"response_mime_type": "application/json"}
        )
        if response.parts:
            return json.loads(response.text)
        else:
            return {"mode": "chat", "reply_message": "ì£„ì†¡í•©ë‹ˆë‹¤. ê³„íšì„ ìˆ˜ë¦½í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "tasks": []}
    except Exception as e:
        return {"mode": "chat", "reply_message": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", "tasks": []}

def execute_crawling(tasks):
    """
    ìˆ˜ë¦½ëœ ê³„íš(tasks)ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    all_results = []
    
    with concurrent.futures.ThreadPoolExecutor() as executor:
        future_to_task = {}
        for task in tasks:
            target = task.get("target_source")
            keyword = task.get("keyword")
            options = task.get("options", {})
            
            # ë””ë²„ê¹…: ì „ë‹¬ë˜ëŠ” íŒŒë¼ë¯¸í„° ì¶œë ¥
            print(f"[DEBUG] Crawling Task:")
            print(f"  - Target: {target}")
            print(f"  - Keyword: {keyword}")
            print(f"  - Options: {options}")
            
            # search_community(target_source, keyword, **kwargs)
            # ê¸°ë³¸ì ìœ¼ë¡œ 1~2í˜ì´ì§€ë§Œ ê¸ë„ë¡ ì„¤ì • (ì†ë„ ìœ„í•´)
            future = executor.submit(search_community, target, keyword, **options)
            future_to_task[future] = task
            
        for future in concurrent.futures.as_completed(future_to_task):
            task = future_to_task[future]
            try:
                df = future.result()
                print(f"[DEBUG] Crawling result for {task.get('target_source')}: {len(df)} rows")
                if not df.empty:
                    # ì¶œì²˜ í‘œê¸°ë¥¼ ìœ„í•´ ì»¬ëŸ¼ ì¶”ê°€
                    df["Source"] = task.get("target_source")
                    df["Keyword"] = task.get("keyword")
                    all_results.append(df)
                else:
                    print(f"[DEBUG] Empty DataFrame returned for {task.get('target_source')}")
            except Exception as e:
                st.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({task}): {e}")
                print(f"[DEBUG] Exception during crawling: {e}")

    if all_results:
        # [ìˆ˜ì •] ì—¬ëŸ¬ ì†ŒìŠ¤ì˜ ë°ì´í„°ë¥¼ ê³ ë¥´ê²Œ ì„ê¸° (Interleaving)
        # ê° ë°ì´í„°í”„ë ˆì„ì— ìˆœìœ„(Rank)ë¥¼ ë§¤ê²¨ì„œ, 1ë“±ë¼ë¦¬, 2ë“±ë¼ë¦¬ ëª¨ì´ë„ë¡ ì •ë ¬
        for df in all_results:
            df['__rank'] = range(len(df))
        
        final_df = pd.concat(all_results, ignore_index=True)
        # Rank ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì†ŒìŠ¤ ê°„ ê· í˜• ë§ì¶¤ (ì˜ˆ: DC 1ìœ„ -> Arca 1ìœ„ -> DC 2ìœ„ ...)
        final_df = final_df.sort_values('__rank').drop(columns=['__rank']).reset_index(drop=True)
        
        return final_df
    else:
        return pd.DataFrame()

def generate_report(user_input, df):
    """
    ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë³´ê³ ì„œë¥¼ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
    (ìˆ˜ì •: í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë° + íˆë“  JSON ë°ì´í„° ì „ì†¡ ë°©ì‹ ì ìš©)
    """
    model = get_gemini_model()
    
    if df.empty:
        return None
        
    summary_text = ""
    
    # ì»¬ëŸ¼ëª… ì²˜ë¦¬ (ëŒ€ì†Œë¬¸ì ë¬´ê´€í•˜ê²Œ ë™ì‘í•˜ë„ë¡ ì•ˆì „ì¥ì¹˜)
    cols = {c.lower(): c for c in df.columns}
    title_col = cols.get('title', 'Title')
    content_col = cols.get('content', 'Content')

    # enumerateë¥¼ ì‚¬ìš©í•˜ì—¬ 1ë²ˆë¶€í„° ì¸ë±ìŠ¤ ë¶€ì—¬
    for i, (idx, row) in enumerate(df.head(30).iterrows()):
        title = row.get(title_col, "No Title")
        # [ìˆ˜ì •] ë³¸ë¬¸ 150ì ì œí•œ (ì´ë¯¸ ì˜ë ¤ìˆê² ì§€ë§Œ ì•ˆì „ì¥ì¹˜ ë° í”„ë¡¬í”„íŠ¸ ìµœì í™”)
        content = str(row.get(content_col, ""))[:150]
        # IDë¥¼ 1ë¶€í„° ì‹œì‘í•˜ëŠ” ìˆœë²ˆìœ¼ë¡œ ë§¤í•‘í•˜ì—¬ í”„ë¡¬í”„íŠ¸ì— ì „ë‹¬
        summary_text += f"[ID: {i + 1}] {title}: {content}\n"
        
    prompt = f"""
    ë‹¹ì‹ ì€ ì»¤ë®¤ë‹ˆí‹° ì—¬ë¡  ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    
    [ì‚¬ìš©ì ì§ˆë¬¸]
    {user_input}
    
    [ìˆ˜ì§‘ëœ ë°ì´í„° ìš”ì•½ (ID í¬í•¨)]
    {summary_text}
    
    ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
    
    [ë³´ê³ ì„œ í¬í•¨ í•­ëª©]
    1. **3ì¤„ ìš”ì•½**: ì „ì²´ì ì¸ ì—¬ë¡ ì˜ í•µì‹¬ ìš”ì•½
    2. **ê¸ì • ì—¬ë¡ **: ìœ ì €ë“¤ì´ ê¸ì •ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ìš”ì†Œ
    3. **ë¶€ì • ì—¬ë¡ **: ìœ ì €ë“¤ì´ ë¶ˆë§Œì´ë‚˜ ë¹„íŒì„ ì œê¸°í•˜ëŠ” ìš”ì†Œ
    4. **ì£¼ìš” ë…¼ìŸ**: í˜„ì¬ ê°€ì¥ ëœ¨ê±°ìš´ ê°ìë‚˜ ë…¼ìŸê±°ë¦¬
    5. **ì¢…í•© í‰ê°€**: ê²°ë¡  ë° ì œì–¸
    
    [íŠ¹ë³„ ì§€ì‹œì‚¬í•­ - ë°ì´í„° êµ¬ì¡°]
    **ì¤‘ìš”:** ë³´ê³ ì„œ ë³¸ë¬¸ ì‘ì„±ì´ ëª¨ë‘ ëë‚˜ë©´, ë°˜ë“œì‹œ `__REF_DATA__` ë¼ëŠ” êµ¬ë¶„ìë¥¼ ì¶œë ¥í•˜ê³ , ê·¸ ë°”ë¡œ ë’¤ì— ë¶„ì„ì— ê°€ì¥ ë„ì›€ì´ ëœ í•µì‹¬ ê²Œì‹œê¸€ì˜ ID ëª©ë¡ì„ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”. (ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ ì—†ì´ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ ì¶œë ¥)
    
    í˜•ì‹ ì˜ˆì‹œ:
    (ë³´ê³ ì„œ ë‚´ìš©...)
    ... ê°ì‚¬í•©ë‹ˆë‹¤.
    
    __REF_DATA__
    {{"reference_ids": [1, 5, 10]}}
    """
    # __REF_DATA__ ìª½ì€ ì‹¤ì œë¡œ ì±„íŒ…ì— ì¶œë ¥ ì•ˆë˜ë„ë¡ ì„¤ì •ë˜ì–´ìˆìŒ
    # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™” (stream=True)
    # JSON ëª¨ë“œëŠ” ìŠ¤íŠ¸ë¦¬ë° ë·°ë¥¼ ë§ì¹˜ë¯€ë¡œ í•´ì œí•˜ê³  í…ìŠ¤íŠ¸ ëª¨ë“œë¡œ ë°›ìŒ
    return model.generate_content(prompt, stream=True)

# --------------------------------------------------------------------------
# 5. ë©”ì¸ ë¡œì§ 
# --------------------------------------------------------------------------
st.title("ğŸ•µï¸â€â™‚ï¸ Community Insight Bot")
st.caption("AIê°€ ìë™ìœ¼ë¡œ ì»¤ë®¤ë‹ˆí‹°ë¥¼ ì„ ì •í•˜ê³  ì»¤ë®¤ë‹ˆí‹° ê¸°ë°˜ ì •ë³´ì™€ ì—¬ë¡ ì„ ë¶„ì„í•©ë‹ˆë‹¤.")

if "messages" not in st.session_state:
    st.session_state.messages = []
    welcome_msg = "ì•ˆë…•í•˜ì„¸ìš”! ê¶ê¸ˆí•œ ê²Œì„, ì¸ë¬¼, ì´ìŠˆ ë“±ì„ ë¬¼ì–´ë´ì£¼ì„¸ìš”. ì œê°€ ì ì ˆí•œ ì»¤ë®¤ë‹ˆí‹°ë¥¼ ì°¾ì•„ ê´€ë ¨ ë‚´ìš©ì„ ì¢…í•©í•´ë“œë¦´ê²Œìš”. ë‹¨ìˆœí•œ ë‹¨ì–´ ë³´ë‹¤ëŠ” í˜„ì¬ ìƒí™©ì„ ì•Œë ¤ì£¼ì‹œë©´ ë” í™•ì‹¤í•œ ê²°ê³¼ë¥¼ ì°¾ì•„ë“œë¦´ ìˆ˜ ìˆì–´ìš”!"
    st.session_state.messages.append({"role": "assistant", "content": welcome_msg})

for message in st.session_state.messages:
    avatar_img = "assets/purple_avatar.png" if message["role"] == "assistant" else None
    with st.chat_message(message["role"], avatar=avatar_img):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ë¬´ì—‡ì„ ë¶„ì„í•´ ë“œë¦´ê¹Œìš”?"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # AI ì‘ë‹µ ìƒì„± ì‹œì‘
    with st.chat_message("assistant", avatar="assets/purple_avatar.png"):
        message_placeholder = st.empty()
        full_response = ""
        
        with st.status("ğŸ¤” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...", expanded=True) as status:
            
            # [Step 1] ê²€ìƒ‰ ê³„íš ìˆ˜ë¦½
            plan = get_search_plan(prompt)
            mode = plan.get("mode", "chat")
            
            # ë””ë²„ê¹…: Geminiê°€ ìƒì„±í•œ ê³„íš ì¶œë ¥
            print(f"[DEBUG] Gemini Plan Generated:")
            print(f"  - Mode: {mode}")
            print(f"  - Reply Message: {plan.get('reply_message', 'N/A')}")
            print(f"  - Tasks: {plan.get('tasks', [])}")
            
            if mode == "search":
                tasks = plan.get("tasks", [])
                
                # ê³„íš ë‚´ìš© í‘œì‹œ
                task_summary = []
                for t in tasks:
                    target = t.get('target_source')
                    keyword = t.get('keyword')
                    task_summary.append(f"{target.upper()}: '{keyword}'")
                
                status.write(f"ğŸ“‹ **ê²€ìƒ‰ ê³„íš ìˆ˜ë¦½ ì™„ë£Œ**: {', '.join(task_summary)}")
                status.update(label="ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤...", state="running")
                
                # [Step 2] í¬ë¡¤ë§ ì‹¤í–‰
                raw_df = execute_crawling(tasks)
                
                if not raw_df.empty:
                    # [ìˆ˜ì •] ìˆ˜ì§‘ ë‹¨ê³„ì—ì„œ ë¯¸ë¦¬ ë³¸ë¬¸ ê¸¸ì´ 150ìë¡œ ì œí•œ (ì„±ëŠ¥ í–¥ìƒ)
                    if 'Content' in raw_df.columns:
                        raw_df['Content'] = raw_df['Content'].astype(str).str.slice(0, 150)

                    initial_count = len(raw_df)
                    status.write(f"âœ… ì´ {initial_count}ê±´ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
                    status.update(label="í˜ì˜¤ í‘œí˜„ì„ í•„í„°ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤...", state="running")
                    
                    # [Step 3] í˜ì˜¤ í‘œí˜„ í•„í„°ë§
                    try:
                        clean_df = filter_hate_speech(raw_df)
                        
                        # [ìˆ˜ì •] ìµœì‹  30ê±´ë§Œ ì‚¬ìš©í•˜ë„ë¡ ìë¥´ê¸°
                        target_df = clean_df.head(30)
                        
                        final_count = len(clean_df)
                        filtered_count = initial_count - final_count
                        
                        # [ìˆ˜ì •] í•„í„°ë§ ê²°ê³¼ ë©”ì‹œì§€ êµ¬ì²´í™”
                        msg = f"ğŸ§¹ **í•„í„°ë§ ì™„ë£Œ**: {filtered_count}ê±´ì˜ ë¶€ì ì ˆí•œ ê²Œì‹œë¬¼ì„ ì œì™¸í–ˆìŠµë‹ˆë‹¤. (ë‚¨ì€ ë°ì´í„°: {final_count}ê±´"
                        if final_count > 30:
                            msg += " ì¤‘ ìµœì‹ ì˜ 30ê±´ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.)"
                        else:
                            msg += ")"
                        status.write(msg)
                            
                    except Exception as e:
                        st.warning(f"í•„í„°ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                        clean_df = raw_df
                        target_df = clean_df.head(30)
                    
                    status.update(label="ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...", state="running")
                    
                    # [Step 4] ë³´ê³ ì„œ ì‘ì„± (ìŠ¤íŠ¸ë¦¬ë° + íˆë“  JSON)
                    try:
                        response_stream = generate_report(prompt, target_df)
                        
                        full_buffer = ""
                        json_part = None
                        
                        # ìŠ¤íŠ¸ë¦¬ë° ë£¨í”„
                        for chunk in response_stream:
                            if chunk.text:
                                full_buffer += chunk.text
                                
                                # êµ¬ë¶„ìê°€ ìˆëŠ”ì§€ í™•ì¸
                                if "__REF_DATA__" in full_buffer:
                                    # êµ¬ë¶„ì ì´ì „ê¹Œì§€ë§Œ í™”ë©´ì— ì¶œë ¥ (JSON ë°ì´í„° ìˆ¨ê¹€)
                                    visible_text = full_buffer.split("__REF_DATA__")[0]
                                    message_placeholder.markdown(visible_text + "â–Œ")
                                else:
                                    # êµ¬ë¶„ìê°€ ì—†ìœ¼ë©´ ì „ì²´ ì¶œë ¥
                                    message_placeholder.markdown(full_buffer + "â–Œ")
                        
                        # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ í›„ì²˜ë¦¬
                        parts = full_buffer.split("__REF_DATA__")
                        report_content = parts[0].strip()
                        full_response = report_content # ìµœì¢… ì €ì¥ìš©
                        
                        # JSON íŒŒì‹± ì‹œë„
                        ref_ids = []
                        if len(parts) > 1:
                            try:
                                json_str = parts[1].strip()
                                # í˜¹ì‹œ ëª¨ë¥¼ ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ì œê±°
                                json_str = json_str.replace("```json", "").replace("```", "").strip()
                                json_data = json.loads(json_str)
                                ref_ids = json_data.get("reference_ids", [])
                            except json.JSONDecodeError:
                                print(f"[DEBUG] JSON Parsing failed: {parts[1]}")

                        # [UI êµ¬ì„± 1] AI ì¶”ì²œ ë§í¬ ì„¹ì…˜ ì¶”ê°€
                        if ref_ids:
                            full_response += "\n\n---\n### ğŸ”— AIê°€ ì°¸ê³ í•œ í•µì‹¬ ê²Œì‹œê¸€\n"
                            
                            # URL ë° Title ì»¬ëŸ¼ ì°¾ê¸° (ëŒ€ì†Œë¬¸ì ë¬´ê´€)
                            cols = {c.lower(): c for c in target_df.columns}
                            url_col = cols.get('posturl') or cols.get('url') or cols.get('link')
                            title_col = cols.get('title', 'Title')

                            if url_col:
                                # ref_idëŠ” 1ë¶€í„° ì‹œì‘í•˜ëŠ” ìˆœë²ˆì´ë¯€ë¡œ, ì¸ë±ìŠ¤ëŠ” ref_id - 1
                                for ref_id in ref_ids:
                                    target_idx = ref_id - 1
                                    if 0 <= target_idx < len(target_df):
                                        row = target_df.iloc[target_idx]
                                        full_response += f"- [{row[title_col]}]({row[url_col]})\n"
                            else:
                                st.warning("URL ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ë§í¬ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        
                        # ìµœì¢… ì™„ì„±ëœ í…ìŠ¤íŠ¸ ì¶œë ¥ (ì»¤ì„œ ì œê±°)
                        message_placeholder.markdown(full_response)
                        
                        # [UI êµ¬ì„± 2] ì‚¬ìš©ëœ ì „ì²´ ê²Œì‹œê¸€ ëª©ë¡ ì¶œë ¥ (Status ë‚´ë¶€)
                        if not target_df.empty:
                            st.markdown("---")
                            st.subheader(f"ğŸ“‘ ì‚¬ìš©ëœ ì „ì²´ ê²Œì‹œê¸€ ({len(target_df)}ê±´)")
                            
                            cols = {c.lower(): c for c in target_df.columns}
                            url_col = cols.get('posturl') or cols.get('url') or cols.get('link')
                            title_col = cols.get('title', 'Title')
                            
                            for i, (idx, row) in enumerate(target_df.iterrows()):
                                title_text = row.get(title_col, "No Title")
                                url_text = row.get(url_col, "#")
                                st.markdown(f"**{i + 1}.** [{title_text}]({url_text})")

                    except Exception as e:
                        full_response += f"\n\n(ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)})"
                        message_placeholder.markdown(full_response)
                        print(f"[DEBUG] Report generation error: {e}")

                    # ìƒíƒœì°½ ë‹«ê¸° ë° ë¼ë²¨ ì—…ë°ì´íŠ¸
                    status.update(label="ë¶„ì„ ì™„ë£Œ! (í´ë¦­í•˜ì—¬ ì „ì²´ ìˆ˜ì§‘ ëª©ë¡ í™•ì¸)", state="complete", expanded=False)
                        
                else:
                    full_response = "ğŸ˜¥ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì§ˆë¬¸í•´ ë³´ì‹œê² ì–´ìš”?"
                    message_placeholder.markdown(full_response)
                    status.update(label="ê²€ìƒ‰ ì‹¤íŒ¨", state="error", expanded=False)
            
            else:
                # Chat / Clarify ëª¨ë“œ
                full_response = plan.get("reply_message", "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?")
                message_placeholder.markdown(full_response)
                status.update(label="ëŒ€í™” ëª¨ë“œ", state="complete", expanded=False)
                
    # ì„¸ì…˜ ê¸°ë¡ ì €ì¥
    st.session_state.messages.append({"role": "assistant", "content": full_response})