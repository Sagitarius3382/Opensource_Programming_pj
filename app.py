import os
import json
import time
import pandas as pd
import streamlit as st
import google.generativeai as genai
import concurrent.futures
from dotenv import load_dotenv

# --------------------------------------------------------------------------
# 1. ì´ˆê¸° ì„¤ì • ë° í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# --------------------------------------------------------------------------
load_dotenv()

st.set_page_config(
    page_title="Community Insight Bot",
    page_icon="ğŸ•µï¸â€â™‚ï¸",
    layout="wide"
)

# ìƒˆë¡œìš´ ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from src.crawler_wrapper import search_community
    from src.preprocessor import filter_hate_speech
except ImportError as e:
    st.error(f"í•„ìˆ˜ ëª¨ë“ˆì„ ì„í¬íŠ¸í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    st.stop()

# --------------------------------------------------------------------------
# 2. ì‚¬ì´ë“œë°” ì„¤ì • (ì»¤ë®¤ë‹ˆí‹° ì„ íƒ ì œê±°, API í‚¤ í™•ì¸ ìœ ì§€)
# --------------------------------------------------------------------------
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    
    # API í‚¤ ë° ëª¨ë¸ ì„¤ì • í™•ì¸ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    if not os.getenv("API_KEY"):
        st.error("ğŸš¨ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    if not os.getenv("MODEL"):
        st.warning("âš ï¸ ëª¨ë¸ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
    st.info("AIê°€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ìë™ìœ¼ë¡œ ì»¤ë®¤ë‹ˆí‹°(DC/Arca)ë¥¼ ì„ ì •í•˜ê³  ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    st.markdown("---")
    st.caption("Powered by Google Gemini")

# --------------------------------------------------------------------------
# 3. Gemini ëª¨ë¸ ë¡œë“œ 
# --------------------------------------------------------------------------
@st.cache_resource
def get_gemini_model():
    """
    Gemini ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤. 
    st.cache_resourceë¥¼ ì‚¬ìš©í•˜ì—¬ ì„¸ì…˜ ê°„ ëª¨ë¸ ê°ì²´ë¥¼ ê³µìœ í•©ë‹ˆë‹¤.
    """
    YOUR_API_KEY = os.getenv("API_KEY")
    if not YOUR_API_KEY:
        st.error("ğŸš¨ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()

    YOUR_MODEL = os.getenv("MODEL")
    if not YOUR_MODEL:
        st.error("ëª¨ë¸ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. '.env' íŒŒì¼ì— 'MODEL'ì„ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        st.stop()
        
    genai.configure(api_key=YOUR_API_KEY)
    
    # ì•ˆì „ ì„¤ì •: ëª¨ë“  ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ ì°¨ë‹¨ ì—†ìŒ(BLOCK_NONE)ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì˜¤íƒì§€ ë°©ì§€
    safety_settings = [
        {
            "category": "HARM_CATEGORY_HARASSMENT",
            "threshold": "BLOCK_NONE"
        },
        {
            "category": "HARM_CATEGORY_HATE_SPEECH",
            "threshold": "BLOCK_NONE"
        },
        {
            "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
            "threshold": "BLOCK_NONE"
        },
        {
            "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
            "threshold": "BLOCK_NONE"
        },
    ]
    
    return genai.GenerativeModel(YOUR_MODEL, safety_settings=safety_settings)

# --------------------------------------------------------------------------
# 4. í•µì‹¬ ë¡œì§ í•¨ìˆ˜ 
# --------------------------------------------------------------------------

def get_search_plan(user_input):
    """
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê²€ìƒ‰ ê³„íšì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤.
    """
    model = get_gemini_model()
    
    system_instruction = """
    ë„ˆëŠ” 'ì»¤ë®¤ë‹ˆí‹° ì—¬ë¡  ë¶„ì„ì„ ìœ„í•œ ê²€ìƒ‰ ì„¤ê³„ì'ì•¼. 
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•´ì„œ ì–´ë–¤ ì»¤ë®¤ë‹ˆí‹°(DCInside, ArcaLive)ë¥¼ ì–´ë–¤ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í• ì§€ êµ¬ì²´ì ì¸ ê³„íšì„ ì„¸ì›Œì¤˜.
    
    [í•„ìˆ˜ ê·œì¹™]
    1. ì‚¬ìš©ìê°€ "ì—¬ë¡ ", "ë°˜ì‘", "í‰ê°€" , "ì˜ê²¬" , "ê·¼í™©" ë“±ì„ ë¬¼ìœ¼ë©´ mode="search"ë¡œ ì„¤ì •í•´.
    2. **ê²€ìƒ‰ì–´(keyword)ëŠ” ê³µì‹ ëª…ì¹­ë³´ë‹¤ ì‹¤ì œë¡œ ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ë§ì´ ì“°ì´ëŠ” 'ì€ì–´'ë‚˜ 'ì¤„ì„ë§'ì„ ìš°ì„ ì ìœ¼ë¡œ ì„ íƒí•´.** (ì˜ˆ: ë¸”ë£¨ ì•„ì¹´ì´ë¸Œ -> ë¸”ì•„, ëª°ë£¨ / ë¦¬ê·¸ì˜¤ë¸Œë ˆì „ë“œ -> ë¡¤)
    3. **[íƒ€ê²Ÿ ì„ ì •]** íŠ¹ì • ì‚¬ì´íŠ¸ ì–¸ê¸‰ì´ ì—†ìœ¼ë©´, í•´ë‹¹ ì£¼ì œê°€ í™œë°œí•œ ê³³ì„ ìë™ìœ¼ë¡œ íŒë‹¨í•˜ë˜ **ì˜ ëª¨ë¥´ê² ê±°ë‚˜ ëŒ€ì¤‘ì ì¸ ê²Œì„/ì´ìŠˆë¼ë©´ ["dc", "arca"] ë‘ ê³³ ëª¨ë‘ tasksì— í¬í•¨í•´.**
    4. DCInsideëŠ” 'gallery_id', ArcaLiveëŠ” 'channel_id'ë¥¼ ë°˜ë“œì‹œ ì¶”ë¡ í•´ì„œ optionsì— í¬í•¨í•´ì•¼ í•´. (ëª¨ë¥´ë©´ 'major'ë‚˜ 'breaking' ê°™ì€ ê¸°ë³¸ê°’ì´ë¼ë„ ë„£ì–´)
        - **ì¤‘ìš”:** DCì˜ ê²Œì„/ì„œë¸Œì»¬ì²˜ ì¥ë¥´ëŠ” ëŒ€ë¶€ë¶„ **'minor' (ë§ˆì´ë„ˆ ê°¤ëŸ¬ë¦¬)** íƒ€ì…ì´ì•¼. (ì˜ˆ: ë©”ì´í”Œ -> maplestory_new (minor))
    5. ì‘ë‹µì€ ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•´. (Markdown ì½”ë“œë¸”ëŸ­ ì—†ì´ ìˆœìˆ˜ JSONë§Œ)

    [JSON ì¶œë ¥ í˜•ì‹]
    {
        "mode": "search" | "clarify" | "chat",
        "reply_message": "ì‚¬ìš©ìì—ê²Œ í•  ë§ (ê³„íšì„ ì„¸ì› ë‹¤ë©´ 'ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”, ~ì— ëŒ€í•´ ì•Œì•„ë³´ê³  ìˆìŠµë‹ˆë‹¤.' ë“±)",
        "tasks": [
            {
                "target_source": "dc" | "arca",
                "keyword": "ì€ì–´_ê¸°ë°˜_ê²€ìƒ‰ì–´",
                "options": {
                    "gallery_id": "ì¶”ë¡ ëœ_ê°¤ëŸ¬ë¦¬ID (dc í•„ìˆ˜)",
                    "channel_id": "ì¶”ë¡ ëœ_ì±„ë„ID (arca í•„ìˆ˜)",
                    "gallery_type": "minor" | "major", 
                    "sort_type": "latest"
                }
            }
        ]
    }
    """
    
    try:
        response = model.generate_content(
            f"{system_instruction}\n\nUser Input: {user_input}",
            generation_config={"response_mime_type": "application/json"}
        )
        if response.parts:
            return json.loads(response.text)
        else:
            return {"mode": "chat", "reply_message": "ì£„ì†¡í•©ë‹ˆë‹¤. ê³„íšì„ ìˆ˜ë¦½í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "tasks": []}
    except Exception as e:
        return {"mode": "chat", "reply_message": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", "tasks": []}

def execute_crawling(tasks):
    """
    ìˆ˜ë¦½ëœ ê³„íš(tasks)ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    all_results = []
    
    with concurrent.futures.ThreadPoolExecutor() as executor:
        future_to_task = {}
        for task in tasks:
            target = task.get("target_source")
            keyword = task.get("keyword")
            options = task.get("options", {})
            
            # ë””ë²„ê¹…: ì „ë‹¬ë˜ëŠ” íŒŒë¼ë¯¸í„° ì¶œë ¥
            print(f"[DEBUG] Crawling Task:")
            print(f"  - Target: {target}")
            print(f"  - Keyword: {keyword}")
            print(f"  - Options: {options}")
            
            # search_community(target_source, keyword, start_page, end_page, **kwargs)
            # ê¸°ë³¸ì ìœ¼ë¡œ 1~2í˜ì´ì§€ë§Œ ê¸ë„ë¡ ì„¤ì • (ì†ë„ ìœ„í•´)
            future = executor.submit(search_community, target, keyword, 1, 2, **options)
            future_to_task[future] = task
            
        for future in concurrent.futures.as_completed(future_to_task):
            task = future_to_task[future]
            try:
                df = future.result()
                print(f"[DEBUG] Crawling result for {task.get('target_source')}: {len(df)} rows")
                if not df.empty:
                    # ì¶œì²˜ í‘œê¸°ë¥¼ ìœ„í•´ ì»¬ëŸ¼ ì¶”ê°€
                    df["Source"] = task.get("target_source")
                    df["Keyword"] = task.get("keyword")
                    all_results.append(df)
                else:
                    print(f"[DEBUG] Empty DataFrame returned for {task.get('target_source')}")
            except Exception as e:
                st.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({task}): {e}")
                print(f"[DEBUG] Exception during crawling: {e}")

    if all_results:
        final_df = pd.concat(all_results, ignore_index=True)
        return final_df
    else:
        return pd.DataFrame()

def generate_report(user_input, df):
    """
    ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
    """
    model = get_gemini_model()
    
    if df.empty:
        return "ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ì–´ ë³´ê³ ì„œë¥¼ ì‘ì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
    # í† í° ì ˆì•½ì„ ìœ„í•´ ìƒìœ„ 30ê°œ ì •ë„ë§Œ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
    summary_text = ""
    # ì»¬ëŸ¼ëª… ëŒ€ì†Œë¬¸ì í˜¸í™˜ì„±ì„ ìœ„í•´ ì²˜ë¦¬
    cols = df.columns
    title_col = next((c for c in cols if c.lower() == 'title'), 'title')
    content_col = next((c for c in cols if c.lower() == 'content'), 'content')

    for idx, row in df.head(30).iterrows():
        title = row.get(title_col, "No Title")
        content = str(row.get(content_col, ""))[:100]
        summary_text += f"- {title}: {content}\n"
        
    prompt = f"""
    ë‹¹ì‹ ì€ ì»¤ë®¤ë‹ˆí‹° ì—¬ë¡  ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    
    [ì‚¬ìš©ì ì§ˆë¬¸]
    {user_input}
    
    [ìˆ˜ì§‘ëœ ë°ì´í„° ìš”ì•½]
    {summary_text}
    
    ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
    ë‹¤ìŒ í•­ëª©ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”:
    1. **3ì¤„ ìš”ì•½**: ì „ì²´ì ì¸ ì—¬ë¡ ì˜ í•µì‹¬ ìš”ì•½
    2. **ê¸ì • ì—¬ë¡ **: ìœ ì €ë“¤ì´ ê¸ì •ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ìš”ì†Œ
    3. **ë¶€ì • ì—¬ë¡ **: ìœ ì €ë“¤ì´ ë¶ˆë§Œì´ë‚˜ ë¹„íŒì„ ì œê¸°í•˜ëŠ” ìš”ì†Œ
    4. **ì£¼ìš” ë…¼ìŸ**: í˜„ì¬ ê°€ì¥ ëœ¨ê±°ìš´ ê°ìë‚˜ ë…¼ìŸê±°ë¦¬
    5. **ì¢…í•© í‰ê°€**: ê²°ë¡  ë° ì œì–¸
    """
    
    return model.generate_content(prompt, stream=True)

# --------------------------------------------------------------------------
# 5. ë©”ì¸ ë¡œì§ 
# --------------------------------------------------------------------------
st.title("ğŸ•µï¸â€â™‚ï¸ Community Insight Bot")
st.caption("AIê°€ ìë™ìœ¼ë¡œ ì»¤ë®¤ë‹ˆí‹°ë¥¼ ì„ ì •í•˜ê³  ì—¬ë¡ ì„ ë¶„ì„í•©ë‹ˆë‹¤.")

if "messages" not in st.session_state:
    st.session_state.messages = []
    welcome_msg = "ì•ˆë…•í•˜ì„¸ìš”! ê¶ê¸ˆí•œ ê²Œì„, ì¸ë¬¼, ì´ìŠˆ ë“±ì„ ë¬¼ì–´ë´ì£¼ì„¸ìš”. ì œê°€ ì•Œì•„ì„œ ì ì ˆí•œ ì»¤ë®¤ë‹ˆí‹°ë¥¼ ì°¾ì•„ ë¶„ì„í•´ë“œë¦´ê²Œìš”."
    st.session_state.messages.append({"role": "assistant", "content": welcome_msg})

for message in st.session_state.messages:
    avatar_img = "assets/purple_avatar.png" if message["role"] == "assistant" else None
    with st.chat_message(message["role"], avatar=avatar_img):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ë¬´ì—‡ì„ ë¶„ì„í•´ ë“œë¦´ê¹Œìš”?"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # AI ì‘ë‹µ ìƒì„± ì‹œì‘
    with st.chat_message("assistant", avatar="assets/purple_avatar.png"):
        message_placeholder = st.empty()
        full_response = ""
        
        with st.status("ğŸ¤” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...", expanded=True) as status:
            
            # [Step 1] ê²€ìƒ‰ ê³„íš ìˆ˜ë¦½
            plan = get_search_plan(prompt)
            mode = plan.get("mode", "chat")
            
            # ë””ë²„ê¹…: Geminiê°€ ìƒì„±í•œ ê³„íš ì¶œë ¥
            print(f"[DEBUG] Gemini Plan Generated:")
            print(f"  - Mode: {mode}")
            print(f"  - Reply Message: {plan.get('reply_message', 'N/A')}")
            print(f"  - Tasks: {plan.get('tasks', [])}")
            
            if mode == "search":
                tasks = plan.get("tasks", [])
                
                # ê³„íš ë‚´ìš© í‘œì‹œ
                task_summary = []
                for t in tasks:
                    target = t.get('target_source')
                    keyword = t.get('keyword')
                    task_summary.append(f"{target.upper()}: '{keyword}'")
                
                status.write(f"ğŸ“‹ **ê²€ìƒ‰ ê³„íš ìˆ˜ë¦½ ì™„ë£Œ**: {', '.join(task_summary)}")
                status.update(label="ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤...", state="running")
                
                # [Step 2] í¬ë¡¤ë§ ì‹¤í–‰
                raw_df = execute_crawling(tasks)
                
                if not raw_df.empty:
                    initial_count = len(raw_df)
                    status.write(f"âœ… ì´ {initial_count}ê±´ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
                    status.update(label="í˜ì˜¤ í‘œí˜„ì„ í•„í„°ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤...", state="running")
                    
                    # [Step 3] í˜ì˜¤ í‘œí˜„ í•„í„°ë§
                    try:
                        clean_df = filter_hate_speech(raw_df)
                        final_count = len(clean_df)
                        filtered_count = initial_count - final_count
                        
                        if filtered_count > 0:
                            status.write(f"ğŸ§¹ **í•„í„°ë§ ì™„ë£Œ**: {filtered_count}ê±´ì˜ ë¶€ì ì ˆí•œ ê²Œì‹œë¬¼ì„ ì œì™¸í–ˆìŠµë‹ˆë‹¤. (ë‚¨ì€ ë°ì´í„°: {final_count}ê±´)")
                        else:
                            status.write("âœ¨ í•„í„°ë§ëœ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤. (ê¹¨ë—í•œ ë°ì´í„°)")
                            
                    except Exception as e:
                        st.warning(f"í•„í„°ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                        clean_df = raw_df
                    
                    status.update(label="ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...", state="running")
                    
                    # [Step 4] ë³´ê³ ì„œ ì‘ì„±
                    response_stream = generate_report(prompt, clean_df)
                    
                    try:
                        for chunk in response_stream:
                            if chunk.parts:
                                full_response += chunk.text
                                message_placeholder.markdown(full_response + "â–Œ")
                    except Exception as e:
                        full_response += f"\n\n(ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)})"
                    
                    message_placeholder.markdown(full_response)
                    status.update(label="ë¶„ì„ ì™„ë£Œ!", state="complete", expanded=False)
                    
                    # [Step 5] ì›ë³¸ ë°ì´í„° í™•ì¸ (Expander)
                    with st.expander("ğŸ“Š ìˆ˜ì§‘ëœ ì›ë³¸ ë°ì´í„° í™•ì¸"):
                        st.dataframe(clean_df, use_container_width=True)
                        
                else:
                    full_response = "ğŸ˜¥ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì§ˆë¬¸í•´ ë³´ì‹œê² ì–´ìš”?"
                    message_placeholder.markdown(full_response)
                    status.update(label="ê²€ìƒ‰ ì‹¤íŒ¨", state="error", expanded=False)
            
            else:
                # Chat / Clarify ëª¨ë“œ
                full_response = plan.get("reply_message", "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?")
                message_placeholder.markdown(full_response)
                status.update(label="ëŒ€í™” ëª¨ë“œ", state="complete", expanded=False)
                
    # ì„¸ì…˜ ê¸°ë¡ ì €ì¥
    st.session_state.messages.append({"role": "assistant", "content": full_response})
